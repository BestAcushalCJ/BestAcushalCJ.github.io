<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>大山</title>
  
  <subtitle>这是一个热爱技术、热爱烹饪的肥宅</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-06-29T08:42:02.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>大山</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>3Blue1Brown follow笔记-1</title>
    <link href="http://yoursite.com/3Blue1Brown_note_02/"/>
    <id>http://yoursite.com/3Blue1Brown_note_02/</id>
    <published>2019-08-04T19:55:23.000Z</published>
    <updated>2019-06-29T08:42:02.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>发明新数学是怎样一种体验？</p></blockquote><p>长度为1的数轴可以表示为如下的分割：<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/3Blue1Brown/01/pic01.png" alt="pic01"><br>写成通式表示为：<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/3Blue1Brown/01/pic02.png" alt="pic02"><br>尽管上式的发现过程表明该式只在0~1的范围内有意义，但实际上p值取任意除1外的数，该式右边仍然成立。<br>例如代入-1，结果如下：<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/3Blue1Brown/01/pic03.png" alt="pic03"><br>该式在趋于无限时以相同的概率获得结果1和0，则其结果可以说是一个期望。<br>代入2时，结果如下：<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/3Blue1Brown/01/pic04.png" alt="pic04"><br>这与我的平时认知毫不相符。它之所以看起来不合理，是因为常规的距离定义类似于曼哈顿距离，dist(A,B)=|B-A|。<br>在这里，转而使用“2进度量”来作为距离的量度，即dist(A,B)=1/|B-A|。更为通用的是“p进度量”，p可以表示任何素数，如今已成为数论中十分重要的概念。<br>使用“2进度量”，2的所有幂之和等于-1就说的通了。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/3Blue1Brown/01/pic05.png" alt="pic05"><br>1，3，7，15，31等数全都趋近于-1。<br>最后，Grant提出通过发现不严格的真理，引导我们构造出有用的严格术语，从而可以发现更多不严格的真理，循环往复。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/3Blue1Brown/01/pic06.png" alt="pic06"></p><blockquote><p>to be done:了解“p进度数”，在之前从未听过，有点捞。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;发明新数学是怎样一种体验？&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;长度为1的数轴可以表示为如下的分割：&lt;br&gt;&lt;img src=&quot;https://blog-1255898404.cos.ap-shanghai.myqcloud.com/3Blu
      
    
    </summary>
    
      <category term="数学&quot;重修&quot;" scheme="http://yoursite.com/categories/%E6%95%B0%E5%AD%A6-%E9%87%8D%E4%BF%AE/"/>
    
    
      <category term="math" scheme="http://yoursite.com/tags/math/"/>
    
      <category term="note" scheme="http://yoursite.com/tags/note/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow模型的保存和加载</title>
    <link href="http://yoursite.com/test/tensorflow_model_save_restore/"/>
    <id>http://yoursite.com/test/tensorflow_model_save_restore/</id>
    <published>2019-08-04T19:55:23.000Z</published>
    <updated>2019-04-22T14:13:48.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="1-Tensorflow的模型到底是什么样的？"><a href="#1-Tensorflow的模型到底是什么样的？" class="headerlink" title="1.Tensorflow的模型到底是什么样的？"></a>1.Tensorflow的模型到底是什么样的？</h4><p>在训练好一个神经网络模型之后，我们通常希望将它保存下来，方便以后的使用。那么，什么是Tensorflow模型呢？Tensorflow模型主要包含网络图和训练好的各参数值等。所以，Tensorflow模型有两个主要的文件：<br>1）Meta graph：<br>这是一个协议缓冲区(protocol buffer)，它完整地保存了Tensorflow图；即所有的变量、操作、集合等。此文件以 .meta 为拓展名。<br>2）checkpoint 文件集合：<br>包含weights、biases、gradients 和其他所有变量的值。从Tensorflow 0.11版本之后，文件列表表示如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mymodel.data-00000-of-00001</span><br><span class="line">mymodel.index</span><br></pre></td></tr></table></figure></p><p>3）checkpoint文件：<br>用于保存最新checkpoint文件保存的记录</p><h4 id="2-保存一个Tensorflow模型"><a href="#2-保存一个Tensorflow模型" class="headerlink" title="2.保存一个Tensorflow模型"></a>2.保存一个Tensorflow模型</h4><p>2.1 创建一个tf.train.Saver()类的实例<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.Saver()</span><br></pre></td></tr></table></figure></p><p>2.2 Tensorflow变量仅存在于session内，所以必须在session内进行保存，可通过调用创建的saver对象的sava方法实现<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saver.save(sess, &apos;mymodel&apos;)</span><br></pre></td></tr></table></figure></p><p>其中sess是session对象，‘mymodel’是对模型的命名。一个完整的例子如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">w1 = tf.Variable(tf.random_normal(shape=[2]), name=&apos;w1&apos;)</span><br><span class="line">w2 = tf.Variable(tf.random_normal(shape=[5]), name=&apos;w2&apos;)</span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">saver.save(sess, &apos;my_test_model&apos;)</span><br></pre></td></tr></table></figure></p><p>如果想在在迭代1000次后保存模型，我们需在对应的步数之后调用 sava 方法，一个简单的示例如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">if step == 1000:</span><br><span class="line">saver.save(sess, &apos;mymodel&apos;, global_step=1000)</span><br></pre></td></tr></table></figure></p><p>在训练过程中，如果想每迭代1000次就保存模型一次。在第一次保存模型时会创建 .meta 文件(第1000次迭代), 并且无需每次都重复创建（也就是无需在第2000、3000…或其他迭代时保存.meta文件）。因为图结构不变，仅需保存迭代后的参数值。因此，当我们不想写入meat-graph时，使用：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saver.save(sess, &apos;mymodel&apos;, global_step=step, write_meta_graph=False)</span><br></pre></td></tr></table></figure></p><p>如果想保存最近的5个模型并且每训练2个小时保存一次，可以使用 max_to_keep 和 keep_checkpoint_every_n_hours参数，如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.Saver(max_to_keep=4, keep_checkpoint_every_n_hours=2)</span><br></pre></td></tr></table></figure></p><h4 id="3-引入一个pretrained模型"><a href="#3-引入一个pretrained模型" class="headerlink" title="3.引入一个pretrained模型"></a>3.引入一个pretrained模型</h4><p>3.1 创建网络<br>可以通过写python代码手工创建原来模型的网络。或者，通过之前保存的 .mate 文件进行网络创建，示例如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.import_meta_graph(&apos;mymodel-1000.meta&apos;)</span><br></pre></td></tr></table></figure></p><p>import_meta_graph会将.meta文件中保存的网络加载到当前网络中，这会创建一个graph/network，但仍需加载已训练的各参数值。<br>3.2 加载参数<br>可以通过调用tf.train.Saver()类的restore方法来加载参数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">with tf.Session() as sess:</span><br><span class="line">  new_saver = tf.train.import_meta_graph(&apos;mymodel-1000.meta&apos;)</span><br><span class="line">  new_saver.restore(sess, tf.train.latest_checkpoint(&apos;./&apos;))</span><br></pre></td></tr></table></figure></p><p>这样，像w1、w2这些tensors的值就加载进来了并且可以进行访问：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">with tf.Session() as sess:    </span><br><span class="line">    saver = tf.train.import_meta_graph(&apos;mymodel-1000.meta&apos;)</span><br><span class="line">    saver.restore(sess,tf.train.latest_checkpoint(&apos;./&apos;))</span><br><span class="line">    print(sess.run(&apos;w1:0&apos;))</span><br></pre></td></tr></table></figure></p><h4 id="4-如何使用加载后的模型"><a href="#4-如何使用加载后的模型" class="headerlink" title="4.如何使用加载后的模型"></a>4.如何使用加载后的模型</h4><p>下面是一个恢复预先训练过的模型，并将其用于预测、微调或进一步培训的示例。无论何时使用TensorFlow，都要定义一个图，表示输入示例（训练数据）和一些超参数（如学习率、全局步骤等）。使用占位符输入所有培训数据和超参数是一种标准实践。让我们用占位符构建一个小型网络并保存它。注意，保存网络时，不会保存占位符的值。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">#Prepare to feed input, i.e. feed_dict and placeholders</span><br><span class="line">w1 = tf.placeholder(&quot;float&quot;, name=&quot;w1&quot;)</span><br><span class="line">w2 = tf.placeholder(&quot;float&quot;, name=&quot;w2&quot;)</span><br><span class="line">b1= tf.Variable(2.0,name=&quot;bias&quot;)</span><br><span class="line">feed_dict =&#123;w1:4,w2:8&#125;</span><br><span class="line"></span><br><span class="line">#Define a test operation that we will restore</span><br><span class="line">w3 = tf.add(w1,w2)</span><br><span class="line">w4 = tf.multiply(w3,b1,name=&quot;op_to_restore&quot;)</span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">#Create a saver object which will save all the variables</span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line">#Run the operation by feeding input</span><br><span class="line">print sess.run(w4,feed_dict)</span><br><span class="line">#Prints 24 which is sum of (w1+w2)*b1</span><br><span class="line"></span><br><span class="line">#Now, save the graph</span><br><span class="line">saver.save(sess, &apos;mymodel&apos;,global_step=1000)</span><br></pre></td></tr></table></figure></p><p>现在，当想要恢复它时，不仅要恢复图表和权重，还要准备一个新的feed-dict，它将把新的训练数据传送到网络。可通过graph.get_tensor_by_name()获得graph中对这些保存的操作和占位符变量的引用。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#How to access saved variable/Tensor/placeholders</span><br><span class="line">w1 = graph.get_tensor_by_name(&quot;w1:0&quot;)</span><br><span class="line"></span><br><span class="line"># How to access saved operation</span><br><span class="line">op_to_restore = graph.get_tensor_by_name(&quot;op_to_restore:0&quot;)</span><br></pre></td></tr></table></figure></p><p>如果只想用不同的数据运行同一个网络，可以简单地通过feed-dict将新数据传递到网络：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">sess=tf.Session()    </span><br><span class="line">#First let&apos;s load meta graph and restore weights</span><br><span class="line">saver = tf.train.import_meta_graph(&apos;mymodel-1000.meta&apos;)</span><br><span class="line">saver.restore(sess,tf.train.latest_checkpoint(&apos;./&apos;))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Now, let&apos;s access and create placeholders variables and</span><br><span class="line"># create feed-dict to feed new data</span><br><span class="line"></span><br><span class="line">graph = tf.get_default_graph()</span><br><span class="line">w1 = graph.get_tensor_by_name(&quot;w1:0&quot;)</span><br><span class="line">w2 = graph.get_tensor_by_name(&quot;w2:0&quot;)</span><br><span class="line">feed_dict =&#123;w1:13.0,w2:17.0&#125;</span><br><span class="line"></span><br><span class="line">#Now, access the op that you want to run.</span><br><span class="line">op_to_restore = graph.get_tensor_by_name(&quot;op_to_restore:0&quot;)</span><br><span class="line"></span><br><span class="line">print sess.run(op_to_restore,feed_dict)</span><br><span class="line">#This will print 60 which is calculated</span><br><span class="line">#using new values of w1 and w2 and saved value of b1.</span><br></pre></td></tr></table></figure></p><p>如果想通过添加更多的网络层来向图添加更多的操作，然后对其进行训练，一个简单的示例如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">sess=tf.Session()    </span><br><span class="line">#First let&apos;s load meta graph and restore weights</span><br><span class="line">saver = tf.train.import_meta_graph(&apos;mymodel-1000.meta&apos;)</span><br><span class="line">saver.restore(sess,tf.train.latest_checkpoint(&apos;./&apos;))</span><br><span class="line"></span><br><span class="line"># Now, let&apos;s access and create placeholders variables and</span><br><span class="line"># create feed-dict to feed new data</span><br><span class="line"></span><br><span class="line">graph = tf.get_default_graph()</span><br><span class="line">w1 = graph.get_tensor_by_name(&quot;w1:0&quot;)</span><br><span class="line">w2 = graph.get_tensor_by_name(&quot;w2:0&quot;)</span><br><span class="line">feed_dict =&#123;w1:13.0,w2:17.0&#125;</span><br><span class="line"></span><br><span class="line">#Now, access the op that you want to run.</span><br><span class="line">op_to_restore = graph.get_tensor_by_name(&quot;op_to_restore:0&quot;)</span><br><span class="line"></span><br><span class="line">#Add more to the current graph</span><br><span class="line">add_on_op = tf.multiply(op_to_restore,2)</span><br><span class="line"></span><br><span class="line">print sess.run(add_on_op,feed_dict)</span><br><span class="line">#This will print 120.</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;1-Tensorflow的模型到底是什么样的？&quot;&gt;&lt;a href=&quot;#1-Tensorflow的模型到底是什么样的？&quot; class=&quot;headerlink&quot; title=&quot;1.Tensorflow的模型到底是什么样的？&quot;&gt;&lt;/a&gt;1.Tensorflow的模型到底
      
    
    </summary>
    
      <category term="教程" scheme="http://yoursite.com/categories/%E6%95%99%E7%A8%8B/"/>
    
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>3Blue1Brown follow笔记-1</title>
    <link href="http://yoursite.com/3Blue1Brown_note_01/"/>
    <id>http://yoursite.com/3Blue1Brown_note_01/</id>
    <published>2019-06-29T03:39:33.890Z</published>
    <updated>2019-06-29T08:28:56.484Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>发明新数学是怎样一种体验？</p></blockquote><p>长度为1的数轴可以表示为如下的分割：<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/3Blue1Brown/01/pic01.png" alt="pic01"><br>写成通式表示为：<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/3Blue1Brown/01/pic02.png" alt="pic02"><br>尽管上式的发现过程表明该式只在0~1的范围内有意义，但实际上p值取任意除1外的数，该式右边仍然成立。<br>例如代入-1，结果如下：<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/3Blue1Brown/01/pic03.png" alt="pic03"><br>该式在趋于无限时以相同的概率获得结果1和0，则其结果可以说是一个期望。<br>代入2时，结果如下：<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/3Blue1Brown/01/pic04.png" alt="pic04"><br>这与我的平时认知毫不相符。它之所以看起来不合理，是因为常规的距离定义类似于曼哈顿距离，dist(A,B)=|B-A|。<br>在这里，转而使用“2进度量”来作为距离的量度，即dist(A,B)=1/|B-A|。更为通用的是“p进度量”，p可以表示任何素数，如今已成为数论中十分重要的概念。<br>使用“2进度量”，2的所有幂之和等于-1就说的通了。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/3Blue1Brown/01/pic05.png" alt="pic05"><br>1，3，7，15，31等数全都趋近于-1。<br>最后，Grant提出通过发现不严格的真理，引导我们构造出有用的严格术语，从而可以发现更多不严格的真理，循环往复。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/3Blue1Brown/01/pic06.png" alt="pic06"></p><blockquote><p>to be done:了解“p进度数”，在之前从未听过，有点捞。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;发明新数学是怎样一种体验？&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;长度为1的数轴可以表示为如下的分割：&lt;br&gt;&lt;img src=&quot;https://blog-1255898404.cos.ap-shanghai.myqcloud.com/3Blu
      
    
    </summary>
    
      <category term="数学&quot;重修&quot;" scheme="http://yoursite.com/categories/%E6%95%B0%E5%AD%A6-%E9%87%8D%E4%BF%AE/"/>
    
    
      <category term="math" scheme="http://yoursite.com/tags/math/"/>
    
      <category term="note" scheme="http://yoursite.com/tags/note/"/>
    
  </entry>
  
  <entry>
    <title>毕业总结</title>
    <link href="http://yoursite.com/graduation/"/>
    <id>http://yoursite.com/graduation/</id>
    <published>2019-06-24T09:52:22.711Z</published>
    <updated>2019-06-24T10:15:29.268Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>江湖路远，有缘再见</p></blockquote><p>6月5日答辩结束时，那种3个月以来的如释重负让我以为这就是毕业的感觉。坐在出租车上从东院赶回余家头校区，路上的街景并不熟悉，大学四年没有来过几次这边校区，一直都在余家头摸爬滚打。<br>余家头校区，不如说是余家头男子职业技术学院，夏季天热人多，冬天酷冷路滑，春秋两季只有短短数十天，宿舍没热水，11点以后会断电。种种不如意，分别时确实万般不舍得。</p><blockquote><p>未完待续。。。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;江湖路远，有缘再见&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;6月5日答辩结束时，那种3个月以来的如释重负让我以为这就是毕业的感觉。坐在出租车上从东院赶回余家头校区，路上的街景并不熟悉，大学四年没有来过几次这边校区，一直都在余家头摸爬滚打。&lt;br&gt;
      
    
    </summary>
    
      <category term="随笔" scheme="http://yoursite.com/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="总结" scheme="http://yoursite.com/tags/%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>本科毕设论文写作结构记录</title>
    <link href="http://yoursite.com/paper_writing_structure/"/>
    <id>http://yoursite.com/paper_writing_structure/</id>
    <published>2019-04-22T14:32:33.117Z</published>
    <updated>2019-04-23T13:54:39.755Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h3><h4 id="1-1-研究背景、目的、意义"><a href="#1-1-研究背景、目的、意义" class="headerlink" title="1.1 研究背景、目的、意义"></a>1.1 研究背景、目的、意义</h4><h4 id="1-2-避障的基本理论、通用算法"><a href="#1-2-避障的基本理论、通用算法" class="headerlink" title="1.2 避障的基本理论、通用算法"></a>1.2 避障的基本理论、通用算法</h4><p>这里讲一下研究现状，包括避障和自己使用的算法</p><h5 id="1-2-1-国外研究现状"><a href="#1-2-1-国外研究现状" class="headerlink" title="1.2.1 国外研究现状"></a>1.2.1 国外研究现状</h5><h5 id="1-2-2-国内研究现状"><a href="#1-2-2-国内研究现状" class="headerlink" title="1.2.2 国内研究现状"></a>1.2.2 国内研究现状</h5><h4 id="1-3-本文的主要研究内容"><a href="#1-3-本文的主要研究内容" class="headerlink" title="1.3 本文的主要研究内容"></a>1.3 本文的主要研究内容</h4><p>在这里把做过的事情要地描述一下</p><h4 id="1-4-小结"><a href="#1-4-小结" class="headerlink" title="1.4 小结"></a>1.4 小结</h4><p>把上面讲过的点串一下（每一章的小结都一样）</p><h3 id="2-避障方法"><a href="#2-避障方法" class="headerlink" title="2 避障方法"></a>2 避障方法</h3><h4 id="2-1-原理"><a href="#2-1-原理" class="headerlink" title="2.1 原理"></a>2.1 原理</h4><p>这里主要是避障原理以及用到的算法所必须的先验知识（在下面公式推导中用的到的）</p><h4 id="2-2-方法"><a href="#2-2-方法" class="headerlink" title="2.2 方法"></a>2.2 方法</h4><p>这里主要写只用DDPG来做避障的一个推导</p><h4 id="2-3-我提出的方法"><a href="#2-3-我提出的方法" class="headerlink" title="2.3 我提出的方法"></a>2.3 我提出的方法</h4><p>这里主要写自己修改后的方法，由于做了两次修改，所以在这里分为两个小部分</p><h5 id="2-3-1-fuzzy-ART-DDPG"><a href="#2-3-1-fuzzy-ART-DDPG" class="headerlink" title="2.3.1 fuzzy ART + DDPG"></a>2.3.1 fuzzy ART + DDPG</h5><h5 id="2-3-2-在2-3-1的基础上加入好奇心机制"><a href="#2-3-2-在2-3-1的基础上加入好奇心机制" class="headerlink" title="2.3.2 在2.3.1的基础上加入好奇心机制"></a>2.3.2 在2.3.1的基础上加入好奇心机制</h5><h4 id="2-4-小结"><a href="#2-4-小结" class="headerlink" title="2.4 小结"></a>2.4 小结</h4><h3 id="3-实验验证"><a href="#3-实验验证" class="headerlink" title="3 实验验证"></a>3 实验验证</h3><h4 id="3-1-实验设置"><a href="#3-1-实验设置" class="headerlink" title="3.1 实验设置"></a>3.1 实验设置</h4><p>在这里讲一下电脑配置参数、仿真环境设计（可以分点）</p><h4 id="3-2-实验"><a href="#3-2-实验" class="headerlink" title="3.2 实验"></a>3.2 实验</h4><p>分别给出上述3个模型的训练（预测）结果，因为都是一边训练一边预测的</p><h5 id="3-2-1-DDPG"><a href="#3-2-1-DDPG" class="headerlink" title="3.2.1 DDPG"></a>3.2.1 DDPG</h5><h5 id="3-2-2-fuzzy-ART-DDPG"><a href="#3-2-2-fuzzy-ART-DDPG" class="headerlink" title="3.2.2 fuzzy ART + DDPG"></a>3.2.2 fuzzy ART + DDPG</h5><h5 id="3-2-3-fuzzy-ART-DDPG-with-curiosity"><a href="#3-2-3-fuzzy-ART-DDPG-with-curiosity" class="headerlink" title="3.2.3 fuzzy ART + DDPG with curiosity"></a>3.2.3 fuzzy ART + DDPG with curiosity</h5><h4 id="3-3-对比分析、讨论"><a href="#3-3-对比分析、讨论" class="headerlink" title="3.3 对比分析、讨论"></a>3.3 对比分析、讨论</h4><h4 id="3-4-小结"><a href="#3-4-小结" class="headerlink" title="3.4 小结"></a>3.4 小结</h4><h3 id="4-结论"><a href="#4-结论" class="headerlink" title="4 结论"></a>4 结论</h3><h4 id="4-1-总结"><a href="#4-1-总结" class="headerlink" title="4.1 总结"></a>4.1 总结</h4><h4 id="4-2-尚且存在的问题、未来要做的工作（展望）"><a href="#4-2-尚且存在的问题、未来要做的工作（展望）" class="headerlink" title="4.2 尚且存在的问题、未来要做的工作（展望）"></a>4.2 尚且存在的问题、未来要做的工作（展望）</h4>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-Introduction&quot;&gt;&lt;a href=&quot;#1-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1 Introduction&quot;&gt;&lt;/a&gt;1 Introduction&lt;/h3&gt;&lt;h4 id=&quot;1-1-研究背景、目的、意义&quot;&gt;&lt;
      
    
    </summary>
    
      <category term="随笔" scheme="http://yoursite.com/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu16.04配置Gym、Torcs、Mujoco环境</title>
    <link href="http://yoursite.com/RL_env_setup/"/>
    <id>http://yoursite.com/RL_env_setup/</id>
    <published>2019-04-22T11:11:25.339Z</published>
    <updated>2019-04-22T12:24:33.312Z</updated>
    
    <content type="html"><![CDATA[<h4 id="1-安装Torcs"><a href="#1-安装Torcs" class="headerlink" title="1.安装Torcs"></a>1.安装Torcs</h4><h5 id="1-1安装依赖项"><a href="#1-1安装依赖项" class="headerlink" title="1.1安装依赖项"></a>1.1安装依赖项</h5><p>前提：python3.5.2<br>安装xautomation包<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install xautomation</span><br></pre></td></tr></table></figure></p><p>激活自己的anaconda环境（根据自己的环境来）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source activate RL</span><br></pre></td></tr></table></figure></p><p>安装keras以及tensorflow（版本自己决定，我在使用的1.4）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install tensorflow==1.4</span><br><span class="line">pip install keras</span><br></pre></td></tr></table></figure></p><p>安装numpy<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip insall numpy</span><br></pre></td></tr></table></figure></p><h5 id="1-2安装"><a href="#1-2安装" class="headerlink" title="1.2安装"></a>1.2安装</h5><p>1.2.1 下载gym_torcs源码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/ugo-nama-kun/gym_torcs.git</span><br></pre></td></tr></table></figure></p><p>1.2.2 将 gym_torcs/vtorcs-RL-color/src/modules/simu/simuv2/simu.cpp 中第64行替换为if (isnan((float)(car-&gt;ctrl-&gt;gear)) || isinf(((float)(car-&gt;ctrl-&gt;gear)))) car-&gt;ctrl-&gt;gear = 0;<br>1.2.3 执行以下命执行安装命令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd gym_torcs/</span><br><span class="line">cd vtorcs-RL-color</span><br><span class="line">sudo apt-get install libglib2.0-dev libgl1-mesa-dev libglu1-mesa-dev freeglut3-dev libplib-dev libopenal-dev libalut-dev libxi-dev libxmu-dev libxrender-dev libxrandr-dev libpng12-dev</span><br><span class="line">./configure</span><br><span class="line">make</span><br><span class="line">sudo make install</span><br><span class="line">sudo make datainstall</span><br></pre></td></tr></table></figure></p><h5 id="1-3测试"><a href="#1-3测试" class="headerlink" title="1.3测试"></a>1.3测试</h5><p>1.3.1 打开终端，输入：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torcs</span><br></pre></td></tr></table></figure></p><p>1.3.2 若安装成功，会出现图形界面，然后依次点击Race –&gt; Practice –&gt; New Race –&gt; 会看到一个蓝屏输出信息“Initializing Driver scr_server1”。<br>1.3.3 在终端中输入如下命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd gym_torcs</span><br><span class="line">source activate RL</span><br><span class="line">python snakeoil3_gym.py</span><br></pre></td></tr></table></figure></p><p>1.3.4 若安装成功，可看到小车在马路上跑了，可以按F1，出现菜单选项，进行相应的观测。</p><h4 id="2-安装Mujoco"><a href="#2-安装Mujoco" class="headerlink" title="2.安装Mujoco"></a>2.安装Mujoco</h4><p>2.1 首先到<a href="https://www.roboti.us/index.html" target="_blank" rel="noopener">官网</a>下载mjpro150 linux<br>2.2 接下来试用30天，或者学生免费使用一年，访问<a href="https://www.roboti.us/license.html" target="_blank" rel="noopener">这里</a><br>2.3 试用30天，填写信息，分别为姓名、邮件地址、电脑ID<br>2.4 点击Linux，下载getid_linux文件<br>2.5 进入getid_linux文件所在文件夹，输入如下命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod +x getid_linux</span><br><span class="line">./getid_linux</span><br></pre></td></tr></table></figure></p><p>2.6 获得的Computer id填写到上面的文本框中。<br>2.7 Mujoco会往你的注册邮箱发送一个mjkey.txt附件<br>2.8 在home目录下创建隐藏文件夹mujoco，并将刚才下载好的mjpro150 linux安装包解压到这个文件夹下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir ~/.mujoco</span><br><span class="line">cp mjpro150_linux.zip ~/.mujoco</span><br><span class="line">cd ~/.mujoco</span><br><span class="line">unzip mjpro150_linux.zip</span><br></pre></td></tr></table></figure></p><p>2.9 拷贝许可证<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp mjkey.txt ~/.mujoco</span><br><span class="line">cp mjkey.txt ~/.mujoco/mjpro150/bin</span><br></pre></td></tr></table></figure></p><p>2.10 添加环境变量<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gedit ~/.bashrc</span><br><span class="line">export LD_LIBRARY_PATH=~/.mujoco/mjpro150/bin$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br><span class="line">export MUJOCO_KEY_PATH=~/.mujoco$&#123;MUJOCO_KEY_PATH&#125;</span><br></pre></td></tr></table></figure></p><p>2.11 测试Mujoco是否安装成功<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd ~/.mujoco/mjpro150/bin</span><br><span class="line">./simulate ../model/humanoid.xml</span><br></pre></td></tr></table></figure></p><h4 id="3-安装Mujoco-py"><a href="#3-安装Mujoco-py" class="headerlink" title="3.安装Mujoco-py"></a>3.安装Mujoco-py</h4><p>3.1 安装依赖项<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo DEBIAN_FRONTEND=noninteractive apt-get install -y curl git libgl1-mesa-dev libgl1-mesa-glx libglew-dev libosmesa6-dev python3-pip python3-numpy python3-scipy net-tools unzip vim wget xpra xserver-xorg-dev</span><br><span class="line">sudo apt-get clean</span><br><span class="line">rm -rf /var/lib/apt/lists/*</span><br><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure></p><p>3.2 安装patchelf<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo curl -o /usr/local/bin/patchelf https://s3-us-west-2.amazonaws.com/openai-sci-artifacts/manual-builds/patchelf_0.9_amd64.elf</span><br><span class="line">sudo chmod +x /usr/local/bin/patchelf</span><br></pre></td></tr></table></figure></p><p>3.3 下载mujoco-py<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/openai/mujoco-py.git</span><br></pre></td></tr></table></figure></p><p>3.4 安装mujoco-py<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd ~/mujoco-py</span><br><span class="line">cp requirements.txt requirements.dev.txt mujoco_py</span><br><span class="line">cd mujoco_py</span><br><span class="line">source activate Gym</span><br><span class="line">pip install -r requirements.txt</span><br><span class="line">pip install -r requirements.dev.txt</span><br><span class="line">pip install -U &apos;mujoco-py&lt;1.50.2,&gt;=1.50.1&apos;</span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure></p><p>3.5 测试安装是否成成功，在终端输入如下命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">source activate RL</span><br><span class="line">python</span><br><span class="line">import mujoco_py</span><br></pre></td></tr></table></figure></p><p>若没有报错，则mojoco-py安装成功</p><h4 id="安装gym-上述安装不报错，才可以成功进行这一步"><a href="#安装gym-上述安装不报错，才可以成功进行这一步" class="headerlink" title="安装gym(上述安装不报错，才可以成功进行这一步)"></a>安装gym(上述安装不报错，才可以成功进行这一步)</h4><p>1.1 在终端输入如下命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">source activate RL</span><br><span class="line">pip install gym[all]</span><br></pre></td></tr></table></figure></p><p>1.2 测试安装是否成功：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python</span><br><span class="line">import gym</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;1-安装Torcs&quot;&gt;&lt;a href=&quot;#1-安装Torcs&quot; class=&quot;headerlink&quot; title=&quot;1.安装Torcs&quot;&gt;&lt;/a&gt;1.安装Torcs&lt;/h4&gt;&lt;h5 id=&quot;1-1安装依赖项&quot;&gt;&lt;a href=&quot;#1-1安装依赖项&quot; class=&quot;
      
    
    </summary>
    
      <category term="教程" scheme="http://yoursite.com/categories/%E6%95%99%E7%A8%8B/"/>
    
    
      <category term="Ubuntu16" scheme="http://yoursite.com/tags/Ubuntu16/"/>
    
      <category term="RL" scheme="http://yoursite.com/tags/RL/"/>
    
  </entry>
  
  <entry>
    <title>Coursera机器学习笔记(八) - 神经网络(下)</title>
    <link href="http://yoursite.com/machine_learning_note_08/"/>
    <id>http://yoursite.com/machine_learning_note_08/</id>
    <published>2018-10-19T06:27:46.890Z</published>
    <updated>2018-10-20T11:32:18.034Z</updated>
    
    <content type="html"><![CDATA[<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script><script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script><blockquote><p>课程地址：<a href="https://www.coursera.org/learn/machine-learning/home/week/5" target="_blank" rel="noopener">Neural Networks:Learning</a><br>课程Wiki：<a href="https://share.coursera.org/wiki/index.php/ML:Neural_Networks:_Learning" target="_blank" rel="noopener">Neural Networks:Learning</a></p></blockquote><h4 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h4><p>在神经网络中我们使用L来表示总的层数, 例如下图中L=4; 使用$s_l$来表示在第$l$层unit的个数（不包括bias unit）,如下图中$s_1=3$,$s_2=5$,$s3=5$,$s_L=4$。对于一个分类问题要么是二分类(Binary classification)要么是多分类(Multi-class classification)。对于二分类问题, 神经网络只需要一个输出单元; 而对于多分类问题, 需要K个输出单元, 其中K为类的个数。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_08/pic01.png" alt="pic01"><br>这里我们按照多分类对代价函数进行描述. 下图为神经网络的代价函数与logistic回归的代价函数的对比。对于前半部分, 因为神经网络中有K个输出, 所以先要对这K个输出的损失求和; 对于后半部分, 因为每一层(除了输出层)都有一个$\Theta$,所以正则化项要将这些权重都包括进来(当然, 不需要包括bias unit的权重)。<br>如下图所示，$h_\Theta(x)$是一个K维的向量, 即$h_\Theta(x) \in \rm I!R^K$,我们用$(h_\Theta(x))_i$来表示第i个输出值。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_08/pic02.png" alt="pic02"><br>下面这段文字是这个课程<a href="https://share.coursera.org/wiki/index.php/ML:Neural_Networks:_Learning" target="_blank" rel="noopener">wiki</a>对于神经网络代价函数的解释：</p><blockquote><p>the double sum simply adds up the logistic regression costs calculated for each cell in the output layer;<br>the triple sum simply adds up the squares of all the individual Θs in the entire network;<br>the i in the triple sum does not refer to training example i.</p></blockquote><h4 id="逆-反向传播算法"><a href="#逆-反向传播算法" class="headerlink" title="逆/反向传播算法"></a>逆/反向传播算法</h4><p>知道了代价函数之后, 我们还是按照套路来求代价函数的最优解. 同样地, 我们希望使用梯度下降来找到最优解. 想要使用梯度下降当然需要求出”梯度”即偏导项$\frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)$。而计算这个偏导项的过程就叫做逆传播算法或者叫反向传播算法。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_08/pic03.png" alt="pic03"><br>首先我们根据前向传播算法来得到$a^{(1)}$,$a^{(2)}$,$a^{(3)}$,$a^{(4)}$和$z^{(1)}$,$z^{(2)}$,$z^{(3)}$。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_08/pic04.png" alt="pic04"><br>在逆传播算法中我们定义每层的误差<br>$$\delta^{(l)}=\frac{\partial}{\partial z^{(l)}}J(\Theta)$$<br>$\delta_j^{(l)}$表示第$l$层第$j$个节点的误差。为了求出偏导项$\frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)$，我们首先要求每一层的$\delta$(不包括第一层, 因为第一层是输入层)。首先, 对于输出层即第4层(这里我们不考虑正则化):<br>$$\begin{align}<br>\delta_j^{(4)} &amp; = \frac{\partial}{\partial z_i^{(4)}}J(\Theta) \ \ &amp; = \frac{\partial J(\Theta)}{\partial a_i^{(4)}}\frac{\partial a_i^{(4)}}{\partial z_i^{(4)}} \ \ &amp; = -\frac{\partial}{\partial a_i^{(4)}}\sum_{k=1}^K\left[y_kloga_k^{(4)}+(1-y_k)log(1-a_k^{(4)})\right]g’(z_i^{(4)})  \ \ &amp; = -\frac{\partial}{\partial a_i^{(4)}}\left[y_iloga_i^{(4)}+(1-y_i)log(1-a_i^{(4)})\right]g(z_i^{(4)})(1-g(z_i^{(4)}))  \ \ &amp; = \left(\frac{1-y_i}{1-a_i^{(4)}}-\frac{y_i}{a_i^{(4)}}\right)a_i^{(4)}(1-a_i^{(4)}) \ \ &amp; = (1-y_i)a_i^{(4)} - y_i(1-a_i^{(4)}) \ \ &amp; = a_i^{(4)} - y_i \<br>\end{align}$$<br>关于$g’(z_i^{(4)})$的证明见本文最后的补充材料。其中$a_j^{(4)}$就是$(h_\Theta(x))_j$，$j$是输出单元的个数，用向量化的表示为：$\delta^{(4)}=a^{(4)}-y$，对于剩下每层的$\delta_i^{(l)}$如下：<br>$$\begin{align} \delta_i^{(l)} &amp; = \frac{\partial}{\partial z_i^{(l)}}J(\Theta) \ \ &amp; = \sum_{j=1}^{S_j}\frac{\partial J(\Theta)}{\partial z_j^{(l+1)}}\cdot\frac{\partial z_j^{(l+1)}}{\partial a_i^{(l)}}\cdot\frac{\partial a_i^{(l)}}{\partial z_i^{(l)}} \ \ &amp; = \sum_{j=1}^{S_j}\delta_j^{(l+1)}\cdot\Theta_{ij}^{(l)}\cdot g’(z_i^{(l)}) \ \ &amp; = g’(z_i^{(l)})\sum_{j=1}^{S_j}\delta_j^{(l+1)}\cdot\Theta_{ij}^{(l)} \end{align}$$<br>写成向量的形式即为:<br>$$\delta^{(l)}=(\Theta^{(l)})^T\delta^{(l+1)}.*g’(z^{(l)})$$<br>求出来所有的$\delta$以后，可以很容易得到$\frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)=a_i^{(l)}\delta_j^{(l+1)}$，这就是我们要求的偏导项(忽略正则化)。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_08/pic05.png" alt="pic05"><br>上面所有的推导都是基于一个样本的, 现在假设有$m$个样本, 我们可以使用BGD来求解最优值。假设我们有一个训练集如下图所示, 对于所有的$l,i,j$，我们先令$\Delta_{ij}^{(l)}=0$，然后对于每一个样本, 都进行如下图所示的计算并使用$\Delta_{ij}^{(l)}$进行累加。其中$\Delta_{ij}^{(l)}:=\Delta_{ij}^{(l)}+a_j^{(l)}\delta_i^{(l+1)}$用向量的表示形式为:$\Delta^{(l)}:=\Delta^{(l)}+\delta^{(l+1)}(a^{(l)})^T$，最后, 计算出$D_{ij}^{(l)}$如下图所示(注意,ppt上有个小错误, 我们的代价函数应该是$D_{ij}^{(l)}:=\frac{1}{m}(\Delta_{ij}^{(l)}+\lambda\Theta_{ij}^{(l)})$<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_08/pic06.png" alt="pic06"><br>关于BP算法, 可以参考Caltech的<a href="https://www.youtube.com/watch?v=Ih5Mr93E-2c&amp;t=3013s&amp;index=10&amp;list=PLD63A284B7615313A" target="_blank" rel="noopener">Learning from Data</a></p><h4 id="参数调整"><a href="#参数调整" class="headerlink" title="参数调整"></a>参数调整</h4><p>为了使用高级优化算法, 这一节我们讲如何调整参数. 在神经网络中, 参数$\Theta^{(j)}$是一个矩阵, 而在之前利用高级优化算法的课程中, 我们知道$\theta$是一个向量, 这个时候就需要对$\Theta$进行Unrolling。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_08/pic07.png" alt="pic07"><br>如下图所示的神经网络中, (ppt中出现了一些错误, 根据$\Theta_1$,$\Theta_2$,$\Theta_3$，这个神经网络应该是有4层, 并且$s_1=10$,$s_2=10$,$s_3=10$,$s_4=1$)。$\Theta^{(1)}$,$\Theta^{(2)}$,$\Theta^{(3)}$,$D^{(1)}$,$D^{(2)}$,$D^{(3)}$,如下图所示, 在Octave/Matlab中, 我们可以使用如下代码将所有对应的矩阵转化成一个向量：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">thetaVec = [Theta1(:); Theta2(:); Theta3(:)];</span><br><span class="line">DVec = [D1(:); D2(:); D3(:)]</span><br><span class="line">%使用如下代码可以得到原来的矩阵</span><br><span class="line">Theta1 = reshape(thetaVec(1: 110), 10, 11);</span><br><span class="line">Theta2 = reshape(thetaVec(111: 220), 10, 11);</span><br><span class="line">Theta3 = reshape(thetaVec(221: 231), 1, 11);</span><br></pre></td></tr></table></figure></p><p><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_08/pic08.png" alt="pic08"><br>下图为利用unrolling来使用高级优化算法的步骤：<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_08/pic09.png" alt="pic09"></p><h4 id="梯度检查"><a href="#梯度检查" class="headerlink" title="梯度检查"></a>梯度检查</h4><p>由于神经网络的复杂性, 我们在使用梯度下降或者其他的高级优化算法时可能会出现bug, 即使感觉上好像没什么问题。那么如何能有效地检查出问题呢, 这个时候就需要使用Gradient Checking。<br>首先, 如下图所示, 我们使用如下近似：$\frac{d}{d\theta}J(\theta)\approx \frac{J(\theta+\epsilon)-J(\theta-\epsilon)}{2\epsilon}$，通常我们取$\epsilon=10^{-4}$。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_08/pic10.png" alt="pic10"><br>上面的例子中$\theta$是一个实数, 在下图中, $\theta$是一个向量, 此时是对偏导数进行数值估计。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_08/pic11.png" alt="pic11"><br>在实际运用中, 我们使用如下代码来计算gradApprox。然后, 我们将通过逆传播算法计算得来的DVec和gradApprox进行比较, 如果这两个值近似的话, 那么就说明我们的逆传播算法运行地没有问题。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_08/pic12.png" alt="pic12"><br>下图描述了使用Gradient Checking时的步骤。需要主意的是, 在得到gradApprox之后一定要即时关闭Gradient Checking, 因为它会非常大地消耗计算资源。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_08/pic13.png" alt="pic13"></p><h4 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h4><p>在之前的linear regression和logistic regression中我们初始化$\theta$的值为0。在神经网络中也可以这么初始化吗?现初始化$\Theta_{ij}^{(l)}=0$，此时会有$a_1^{(2)}=a_2^{(2)}$,$\delta_1^{(2)}=\delta_2^{(2)}$。这样不论进行多少次更新, 永远会有$a_1^{(2)}=a_2^{(2)}$，也就是说这两个神经元是完全等同的, 这显然不合理, 那么我们应该如何初始化参数呢?<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_08/pic14.png" alt="pic14"><br>对于$\Theta^{(1)} \in \mathbb{R}^{10\times 11}$，使用随机函数来进行初始化：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Theta1 = rand(10, 11)*(2*INIT_EPSILON) - INIT_EPSILON</span><br></pre></td></tr></table></figure></p><p><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_08/pic15.png" alt="pic15"></p><h4 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h4><p>神经网络有许许多多种结构, 我们应该如何选择神经网络的结构？<br>在神经网络中, 有两个是确定的, 那就是输入单元的个数和输出单元的个数。因为前者就是特征的个数（维度）, 而后者是分类的数量。一个合理的默认值为, 有一个隐藏层；或者有多个隐藏层, 并且这些隐藏层单元数量相等。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_08/pic16.png" alt="pic16"></p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_08/pic17.png" alt="pic17"><br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_08/pic18.png" alt="pic18"></p><h4 id="补充材料"><a href="#补充材料" class="headerlink" title="补充材料"></a>补充材料</h4><p>$$\begin{align} \frac{\partial g(z)}{\partial z} &amp; = -\left( \frac{1}{1 + e^{-z}} \right)^2\frac{\partial{}}{\partial{z}} \left(1 + e^{-z} \right) \ \ &amp; = -\left( \frac{1}{1 + e^{-z}} \right)^2e^{-z}\left(-1\right) \ \ &amp; = \left( \frac{1}{1 + e^{-z}} \right) \left( \frac{1}{1 + e^{-z}} \right)\left(e^{-z}\right) \ \ &amp; = \left( \frac{1}{1 + e^{-z}} \right) \left( \frac{e^{-z}}{1 + e^{-z}} \right) \ \ &amp; = \left( \frac{1}{1+e^{-z}}\right)\left( \frac{1+e^{-z}}{1+e^{-z}}-\frac{1}{1+e^{-z}}\right) \ \ &amp; = g(z) \left( 1 - g(z)\right) \ \ \end{align}$$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]]}
});
&lt;/script&gt;

&lt;script type=&quot;te
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
      <category term="Math" scheme="http://yoursite.com/tags/Math/"/>
    
      <category term="Note" scheme="http://yoursite.com/tags/Note/"/>
    
      <category term="Neural Network" scheme="http://yoursite.com/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>Coursera机器学习笔记(七) - 神经网络(上)</title>
    <link href="http://yoursite.com/machine_learning_note_07/"/>
    <id>http://yoursite.com/machine_learning_note_07/</id>
    <published>2018-09-24T06:24:20.581Z</published>
    <updated>2018-10-17T16:41:50.626Z</updated>
    
    <content type="html"><![CDATA[<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script><script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script><blockquote><p>课程地址：<a href="https://www.coursera.org/learn/machine-learning/home/week/4" target="_blank" rel="noopener">Neural Networks:Representation</a><br>课程Wiki：<a href="https://share.coursera.org/wiki/index.php/ML:Neural_Networks:_Representation" target="_blank" rel="noopener">Neural Networks:Representation</a></p></blockquote><h4 id="为什么要使用神经网络"><a href="#为什么要使用神经网络" class="headerlink" title="为什么要使用神经网络"></a>为什么要使用神经网络</h4><p>对于下图所示的分类问题, 我们可以利用高阶项来构造我们的假设函数. 但是, 实际问题往往有很多特征. 例如在房价预测的问题中, 我们可能有100个特征, 如果想要多项式包含所有的二次项那么这个多项式会有5,000个特征(复杂度为$O(n^2)$)。这样就会带来两个问题：1.过拟合 2.消耗大量计算资源. 当然可以使用所有二次项的子集, 例如$x_1^2, x_2^2, x_3^2, …$，但是这样可能又欠拟合. 如果想要包含所有的三次项, 那大概会有170,000个特征, 复杂度为$O(n^3)$。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_07/pic01.png" alt="pic01"></p><a id="more"></a><p>下面考虑一个计算机视觉的问题. 假设我们想训练一个可以识别汽车图片的分类器. 一张图片对于计算机来说就是一堆数字矩阵。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_07/pic02.png" alt="pic02"><br>对于一个50x50的图像会有2,500个像素, 即n=2,500(如果是RGB图的话n就是7500). 如果我们想要包含所有的二次项, 那么特征就是3,000,000个。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_07/pic03.png" alt="pic03"><br>对于这类问题使用logistic回归显然是没法解决的, 这个时候就要用到神经网络(Neural Network)。</p><h4 id="神经网络结构"><a href="#神经网络结构" class="headerlink" title="神经网络结构"></a>神经网络结构</h4><p>下图为一个神经元(neuron), 它的输入为$x_1, x_2, x_3$，有时候为了方便我们添加一个$x_0$，叫做bias unit. 它的输出为$h(\theta)$，$\theta$也叫做weights。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_07/pic04.png" alt="pic04"><br>下图为由多个神经元组成的神经网络. 第一层叫做输入层(input layer), 最后一层叫做输出层(output layer), 中间的叫做隐藏层(hidden layer)。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_07/pic05.png" alt="pic05"><br>注意: 如果在第$j$层有$s_j$个units，在第$j+1$层有$s_{j+1}$个units，那么$\Theta^{(j)}$就是一个$s_{(j+1)} \times (s_j + 1)$的矩阵。(因为前一层增加了一个bias unit)<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_07/pic06.png" alt="pic06"></p><h4 id="前向传播算法"><a href="#前向传播算法" class="headerlink" title="前向传播算法"></a>前向传播算法</h4><p>前向传播算法其实就是由输入层计算出输出的过程, 这里为了提高计算的效率, 我们使用向量化的算法. 首先我们做如下定义:<br>$$z_1^{(2)}=\Theta_{10}^{(1)}x_0+\Theta_{11}^{(1)}x_1+\Theta_{12}^{(1)}x_2+\Theta_{13}^{(1)}x_3$$<br>$$z_2^{(2)}=\Theta_{20}^{(1)}x_0+\Theta_{21}^{(1)}x_1+\Theta_{22}^{(1)}x_2+\Theta_{23}^{(1)}x_3$$<br>$$z_3^{(2)}=\Theta_{30}^{(1)}x_0+\Theta_{31}^{(1)}x_1+\Theta_{32}^{(1)}x_2+\Theta_{33}^{(1)}x_3$$<br>$$z^{(2)} =\begin{bmatrix} z_1^{(2)} \ z_2^{(2)} \ z_3^{(2)} \end{bmatrix}$$<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_07/pic07.png" alt="pic07"><br>这样$z^{(2)}$的计算就可以写成向量计算:$z^{(2)} = \Theta^{(1)}x$，进而有$a^{(2)}=g(z^{(2)})$，为了和后面几层的写法统一, 我们令$a^{(1)}=x$，所以有$z^{(2)} = \Theta^{(1)}a^{(1)}$。<br>我们在隐藏层加上一个额外的$a_{(0)}^{(2)}=1$，得到$a^{(2)}=\begin{bmatrix} a_0^{(2)} \ a_1^{(2)} \ a_2^{(2)} \ a_3^{(2)} \end{bmatrix}$，同理，$z^{(3)}=\Theta^{(2)}a^{(2)}$，最后$h_\Theta(x)=a^{(3)}=g(z^{(3)})$。<br>现在我们先把刚才的神经网络的输入层遮住, 观察剩下部分的结构以及算法我们发现, 其实这一部分其实就是前面所讲的logistic回归. 不同的是, 它的输入$\alpha$是由真正的特征$x$学习得到的，可以把$\alpha$看出新的特征，$x$看成初始特征。这样, 神经网络就相当于通过初始特征学习到新的特征, 再通过新的特征进行logistic回归得到输出结果。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_07/pic08.png" alt="pic08"><br>当然, 神经网络不仅仅是上面一种结构, 下图展示了另一种神经网结构<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_07/pic09.png" alt="pic09"></p><h4 id="神经网络与逻辑运算"><a href="#神经网络与逻辑运算" class="headerlink" title="神经网络与逻辑运算"></a>神经网络与逻辑运算</h4><h5 id="逻辑与-逻辑或"><a href="#逻辑与-逻辑或" class="headerlink" title="逻辑与 逻辑或"></a>逻辑与 逻辑或</h5><p>逻辑与运算, 参数为-30, 20, 20. 结果如下图所示：<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_07/pic10.png" alt="pic10"><br>逻辑或运算, 参数为-10, 20, 20. 结果如下图所示：<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_07/pic11.png" alt="pic11"></p><h5 id="逻辑非"><a href="#逻辑非" class="headerlink" title="逻辑非"></a>逻辑非</h5><p>逻辑非运算, 参数为10, -20. 结果如下图所示：<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_07/pic12.png" alt="pic12"><br>将三个神经单元组成一个神经网络, 可以得到同或运算：<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_07/pic13.png" alt="pic13"></p><h4 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a>多分类</h4><p>下两图为多分类问题：<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_07/pic14.png" alt="pic14"><br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_07/pic15.png" alt="pic15"></p>]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]]}
});
&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; async src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;blockquote&gt;
&lt;p&gt;课程地址：&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/week/4&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Neural Networks:Representation&lt;/a&gt;&lt;br&gt;课程Wiki：&lt;a href=&quot;https://share.coursera.org/wiki/index.php/ML:Neural_Networks:_Representation&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Neural Networks:Representation&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&quot;为什么要使用神经网络&quot;&gt;&lt;a href=&quot;#为什么要使用神经网络&quot; class=&quot;headerlink&quot; title=&quot;为什么要使用神经网络&quot;&gt;&lt;/a&gt;为什么要使用神经网络&lt;/h4&gt;&lt;p&gt;对于下图所示的分类问题, 我们可以利用高阶项来构造我们的假设函数. 但是, 实际问题往往有很多特征. 例如在房价预测的问题中, 我们可能有100个特征, 如果想要多项式包含所有的二次项那么这个多项式会有5,000个特征(复杂度为$O(n^2)$)。这样就会带来两个问题：1.过拟合 2.消耗大量计算资源. 当然可以使用所有二次项的子集, 例如$x_1^2, x_2^2, x_3^2, …$，但是这样可能又欠拟合. 如果想要包含所有的三次项, 那大概会有170,000个特征, 复杂度为$O(n^3)$。&lt;br&gt;&lt;img src=&quot;https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_07/pic01.png&quot; alt=&quot;pic01&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
      <category term="Math" scheme="http://yoursite.com/tags/Math/"/>
    
      <category term="Note" scheme="http://yoursite.com/tags/Note/"/>
    
      <category term="Neural Network" scheme="http://yoursite.com/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>人工智能、机器学习和深度学习</title>
    <link href="http://yoursite.com/AI_ML_DL_comments/"/>
    <id>http://yoursite.com/AI_ML_DL_comments/</id>
    <published>2018-09-16T08:09:26.966Z</published>
    <updated>2018-09-16T08:11:50.589Z</updated>
    
    <content type="html"><![CDATA[<h3 id="人工智能、机器学习和深度学习"><a href="#人工智能、机器学习和深度学习" class="headerlink" title="人工智能、机器学习和深度学习"></a>人工智能、机器学习和深度学习</h3><blockquote><p>机器学习需要解决的问题：如何让计算机可以和人类一样从历史的经验中获取新的知识？</p></blockquote><p>提取特征：如何数字化地表达现实世界中的实体。如何从实体中提取特征，对于机器学习算法的性能有很大影响。<br>例：如果将图书馆中的图书信息存储为结构化的数据，如存在excel表中，那么非常容易通过书名等信息查找一本书是否在图书馆中；如果图书信息存储在非结构化的图片中，那么完成查找任务就会变得十分困难。<br><a id="more"></a><br>同样的数据使用不同的表达方式会极大的影响解决问题的难度。<br>例：在笛卡尔坐标系中表示为类似一个圆形的两种颜色点数据集合，不同颜色的点无法被一条直线划分，但是如果将这些点映射到极角坐标系中，那么使用直线划分就很容易。<br>一旦解决了特征提取和数据表达，很多人工智能任务也就解决了大半。</p><blockquote><p>人工的方式无法很好的抽取实体中的特征，深度学习的核心问题之一就是自动地将简单的特征组合成更加复杂的特征，并使用这些组合特征解决问题。</p></blockquote><blockquote><p>深度学习时机器学习的一个分支。</p></blockquote><p>传统机器学习算法：输入–&gt;人工特征提取–&gt;权重学习–&gt;预测结果<br>深度学习算法：输入–&gt;基础特征提取–&gt;多次复杂特征提取–&gt;权重学习–&gt;预测结果</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;人工智能、机器学习和深度学习&quot;&gt;&lt;a href=&quot;#人工智能、机器学习和深度学习&quot; class=&quot;headerlink&quot; title=&quot;人工智能、机器学习和深度学习&quot;&gt;&lt;/a&gt;人工智能、机器学习和深度学习&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;机器学习需要解决的问题：如何让计算机可以和人类一样从历史的经验中获取新的知识？&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;提取特征：如何数字化地表达现实世界中的实体。如何从实体中提取特征，对于机器学习算法的性能有很大影响。&lt;br&gt;例：如果将图书馆中的图书信息存储为结构化的数据，如存在excel表中，那么非常容易通过书名等信息查找一本书是否在图书馆中；如果图书信息存储在非结构化的图片中，那么完成查找任务就会变得十分困难。&lt;br&gt;
    
    </summary>
    
      <category term="随想" scheme="http://yoursite.com/categories/%E9%9A%8F%E6%83%B3/"/>
    
    
      <category term="人工智能" scheme="http://yoursite.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Coursera机器学习笔记(六) - 正则化</title>
    <link href="http://yoursite.com/machine_learning_note_06/"/>
    <id>http://yoursite.com/machine_learning_note_06/</id>
    <published>2018-09-08T06:53:34.556Z</published>
    <updated>2018-10-15T07:32:56.397Z</updated>
    
    <content type="html"><![CDATA[<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script><script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script><blockquote><p>课程地址：<a href="https://www.coursera.org/learn/machine-learning/home/week/3" target="_blank" rel="noopener">Regularization</a><br>课程Wiki：<a href="https://share.coursera.org/wiki/index.php/ML:Regularization" target="_blank" rel="noopener">Regularization</a></p></blockquote><h4 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h4><p>如下图所示, 使用三种不同的多项式作为假设函数对数据进行拟合, 左一和右一分别为过拟合和欠拟合。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_06/pic01.png" alt="pic01"></p><a id="more"></a><p>对率回归:<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_06/pic02.png" alt="pic02"><br>解决过拟合问题大致分为两种, 一种是减少特征的数量, 可以人工选择一些比较重要的特征留下, 也可以使用模型选择算法(Model selection algorithm,后面的课程会介绍)；另一种就是正则化(Regularization)。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_06/pic03.png" alt="pic03"></p><h4 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h4><p>如图所示的两个假设函数, 其中第二个为过拟合. 那么该如何改变代价函数能够让最中的假设函数不过拟合? 对比两个假设函数我们可以看到, 它们的区别就在于第二个多了两个高阶项. 也就是说, 我们不希望出现后面两个高阶项, 即希望$\theta_3$、$\theta_4$尽可能小。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_06/pic04.png" alt="pic04"><br>通过上面的想法, 我们把$\theta_3$、$\theta_4$放到代价函数中，并且加上很大的权重(1000):<br>$$J(\theta)=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)} )^2+1000\theta_3^2+1000\theta_4^2$$<br>现在如果要最小化代价函数, 那么最后两项也必须得最小. 这个时候, 就有$\theta_3\approx0$，$\theta_4\approx0$，从而这个四次多项式就变成了一个二次多项式, 解决了过拟合的问题。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_06/pic05.png" alt="pic05"><br>对于正则化的一般思路是, 减少特征的数量, 降低模型的复杂度. 所以我们要对每个参数进行惩罚, 从而得到’更简单’的并且可以防止过拟合的模型. 但是在实际问题中我们很难判断哪些特征比较重要, 所以对每一个参数(除了第一个)参数进行惩罚, 将代价函数改为:<br>$$J(\theta)=\frac{1}{2m}\left[\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)} )^2+\lambda\sum_{i=1}^n\theta_i^2\right]$$<br>其中，$\lambda\sum_{i=1}^n\theta_j^2$叫做正则化项(Regularization Term)，$\lambda$叫做正则化参数(Regularization Parameter)。$\lambda$的作用就是在”更好地拟合数据”和”防止过拟合”之间权衡。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_06/pic06.png" alt="pic06"><br>如果$\lambda$过大的话，就会导致$\theta_1$、$\theta_2$、$\theta_3$…近似于0, 这样我们的假设函数就为：$h_\theta(x)=\theta_0$。这时就变成了欠拟合(Underfit). 所以需要选择一个合适的$\lambda$。后面的课程会讲到自动选择合适的$\lambda$的方法。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_06/pic07.png" alt="pic07"></p><h4 id="正则化线性回归"><a href="#正则化线性回归" class="headerlink" title="正则化线性回归"></a>正则化线性回归</h4><p>通过正则化之后的$J(\theta)$我们可以得到对应的梯度下降算法, 如下图所示. 因为我们不对$\theta_0$进行惩罚, 所以将$\theta_0$的规则单独写出来, 其余的参数更新规则如下图第三行公式. 公式前半部分$1-\alpha\frac{\lambda}{m}$是一个比1小一点点的数(教授举了个例子大概是0.99), 而公式的后半部分和没有进行正则化的梯度下降的公式的后半部分是完全一样的. 所以区别就在于前半部分会将$\theta_j$缩小(因为乘了一个小于1的数)。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_06/pic08.png" alt="pic08"><br>同样, 在正规方程中, 我们只需要在公式中加上一部分如下图所示<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_06/pic09.png" alt="pic09"><br>并且对于正则化后的正规方程, 只要$\lambda&gt;0$, 括号里的那一项总是可逆的:<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_06/pic10.png" alt="pic10"></p><h4 id="正则化对率回归"><a href="#正则化对率回归" class="headerlink" title="正则化对率回归"></a>正则化对率回归</h4><p>类似地, 正则化逻辑回归中的代价函数和梯度下降如下图所示<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_06/pic11.png" alt="pic11"><br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_06/pic12.png" alt="pic12"><br>下图是使用正则化的高级优化算法, 只需要在计算jVal时在后面加上一个正则化项以及在梯度后面减去一个$\frac{\lambda}{m}\theta_j$<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_06/pic13.png" alt="pic13"></p>]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]]}
});
&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; async src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;blockquote&gt;
&lt;p&gt;课程地址：&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/week/3&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Regularization&lt;/a&gt;&lt;br&gt;课程Wiki：&lt;a href=&quot;https://share.coursera.org/wiki/index.php/ML:Regularization&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Regularization&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&quot;过拟合&quot;&gt;&lt;a href=&quot;#过拟合&quot; class=&quot;headerlink&quot; title=&quot;过拟合&quot;&gt;&lt;/a&gt;过拟合&lt;/h4&gt;&lt;p&gt;如下图所示, 使用三种不同的多项式作为假设函数对数据进行拟合, 左一和右一分别为过拟合和欠拟合。&lt;br&gt;&lt;img src=&quot;https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_06/pic01.png&quot; alt=&quot;pic01&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
      <category term="Math" scheme="http://yoursite.com/tags/Math/"/>
    
      <category term="Note" scheme="http://yoursite.com/tags/Note/"/>
    
      <category term="Regularization" scheme="http://yoursite.com/tags/Regularization/"/>
    
  </entry>
  
  <entry>
    <title>Coursera机器学习笔记(五) - Logistic Regression</title>
    <link href="http://yoursite.com/machine_learning_note_05/"/>
    <id>http://yoursite.com/machine_learning_note_05/</id>
    <published>2018-09-07T15:59:57.178Z</published>
    <updated>2018-10-14T15:41:55.675Z</updated>
    
    <content type="html"><![CDATA[<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script><script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script><blockquote><p>课程地址：<a href="https://www.coursera.org/learn/machine-learning/home/week/3" target="_blank" rel="noopener">Logistic Regression</a><br>课程Wiki：<a href="https://share.coursera.org/wiki/index.php/ML:Logistic_Regression" target="_blank" rel="noopener">Logistic Regression</a></p></blockquote><h4 id="模型展示"><a href="#模型展示" class="headerlink" title="模型展示"></a>模型展示</h4><h5 id="从线性回归解到对数几率回归"><a href="#从线性回归解到对数几率回归" class="headerlink" title="从线性回归解到对数几率回归"></a>从线性回归解到对数几率回归</h5><p>前面的课程中提到了一些分类问题:<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_05/pic01.png" alt="pic01"></p><a id="more"></a><p>对于乳腺癌的那个例子, 数据集如下所示. 如果使用线性回归来处理这个问题, 我们可能得到这样一个假设函数:<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_05/pic02.png" alt="pic02"><br>然后我们设定一个阈值0.5, 当假设函数的输出大于这个阈值时我们预测$y=1$；当假设函数的输出小于0.5时, 我们预测$y=0$。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_05/pic03.png" alt="pic03"><br>即使我们的问题是一个分类问题, 但是对于上面这个特定的例子, 看上去使用线性回归好像还是挺合理的. 但是如果我们再增加一个数据(下图最右), 使用Linear Regression就会得到如图蓝色线所示的$h_\theta(x)$。<br>这个结果显然是不合理的, 它有很多错误的分类。 直观上来看, 要是能得到图中垂直于横轴的(图中蓝色)线那便是极好的。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_05/pic04.png" alt="pic04"><br>而且即使所有的训练样例的$y = 0或1$, 使用线性回归得到的$h_\theta(x)$也有可能大于1或者小于0。<br>所以我们需要一个能更好地处理分类问题的模型, 即对数几率回归(Logistic Regression/Logit regression). (有些地方翻译成“逻辑回归”或者“逻辑斯蒂格回归”) 需要注意的是这个模型虽然叫regression, 但是它是一个用来解决分类问题的模型。在对率回归(对数几率回归的简称)中,$0\le h_\theta(x)\le1$。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_05/pic05.png" alt="pic05"></p><h5 id="假设函数"><a href="#假设函数" class="headerlink" title="假设函数"></a>假设函数</h5><p>在对率回归中, 假设函数为$h_\theta(x)=g(\theta^Tx)$，其中$g(z)=\frac{1}{1+e^{-z}}$，$g(z)$叫做Sigmoid Function或者对率函数(Logistic Function)。<br>对数几率函数是Sigmoid函数的一种, 它将z值转化为一个接近0或1的y值, 并且其输出值在$z=0$附近变化很陡. Sigmoid函数即形似S的函数, 对率函数是Sigmoid函数最重要的代表。<br>$${h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}}$$<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_05/pic06.png" alt="pic06"><br>我们可以将对率函数的输出理解为当输入为x的时候, y=1的概率. 可以用$h_\theta(x)=P(y=1|x;\theta)$表达. 例如, 在下图中, 我们给定一个x, 它的假设函数的输出为0.7, 我们可以说这个病人的肿瘤为恶性的概率是70%。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_05/pic07.png" alt="pic07"></p><h5 id="决策边界"><a href="#决策边界" class="headerlink" title="决策边界"></a>决策边界</h5><p>如图所示的Sigmoid函数, 我们可以看到, 当$z&gt;0$的时候$g(z)\ge0.5$即预测$y=1$；当$z&lt;0$的时候$g(z)&lt;0.5$即预测$y=0$。<br>而在对率回归中, 我们的$z=\theta^Tx$，所以我们有：当$\theta^Tx&gt;0$时，预测$y=1$；当$\theta^Tx&lt;0$时，预测$y=0$。如下图所示<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_05/pic08.png" alt="pic08"><br>下面举一个例子看一看什么是决策边界. 如下图所示, 左边为数据集. 假设此时我们已经通过训练得到了$\theta=\begin{bmatrix}-3\ 1\ 1\end{bmatrix}$，由上图可知, 当$\theta^Tx&gt;0$时，预测$y=1$，即当$-3+x_1+x_2\ge0$时, 预测$y=1$；也即当$x_1+x_2\ge3$时, 预测$y=1$。我们在坐标中画出$x_1+x_2=3$的图形, 如果数据在这条直线的上方, 我们就预测$y=1$；如果数据在这条直线的下方, 我们就预测$y=0$，我们把$x_1+x_2\ge3$称为决策边界(Decision Boundary)。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_05/pic10.png" alt="pic10"><br>下面看一个比较复杂的决策边界的例子。在线性回归的时候谈到过使用高阶多项式特征, 当然这里我们也可以用。我们添加两个特征一个是$x_1^2$，一个是$x_2^2$，假设我们通过训练得到参数$\theta$，如下图所示。当$-1+x_1^2+x_2^2\ge0$的时候, 预测$y=1$，即$x_1^2+x_2^2\ge1$的时候, 预测$y=1$，画出$x_1^2+x_2^2=1$的图形，在圆内$y=0$，在园外$y=1$。这是一个圆形的决策边界。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_05/pic11.png" alt="pic11"><br>当我们有更高阶的多项式时, 我们会得到更加复杂的决策边界。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_05/pic12.png" alt="pic12"></p><h4 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h4><p>在之前的线性回归中, 我们的代价函数为：<br>$${J(\theta)=\frac{1}{m}\sum_{i=1}^m\frac{1}{2}\left(h_\theta(x^{(i)})-y^{(i)}\right)^2}$$<br>令<br>$$Cost\left(h_\theta(x^{(i)}),y^{(i)}\right)=\frac{1}{2}\left(h_\theta(x^{(i)})-y^{(i)}\right)^2$$<br>简记为<br>$$Cost\left(h_\theta(x),y\right)=\frac{1}{2}\left(h_\theta(x)-y\right)^2$$<br>在线性回归中, 之所以可以使用梯度下降来找到全局最优解是因为代价函数$J(\theta)$是一个凸函数(convex). 但是对于对率回归来说, 假设函数$h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}$是一个较为复杂的非线性函数, 直接带入的话得到的代价函数就不是一个凸函数(non-convex), 如下图左侧部分所示. 这样使用梯度下降只能找到局部最优. 所以现在我们需要构造一个合理的并且是凸函数的代价函数。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_05/pic13.png" alt="pic13"><br>在对数几率回归中, 使用如下的$Cost\left(h_\theta(x),y\right)$<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_05/pic14.png" alt="pic14"><br>当$y=1$时，$Cost\left(h_\theta(x),y\right)$如下图所示，此时, 如果我们$h_\theta(x)=1$，那么$Cost=0$，即当预测结果和真实结果一样时, 我们不对学习算法进行惩罚；但是当结果不一致时, 即$h_\theta(x)→ 0$时, 我们对算法的惩罚趋近于无穷。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_05/pic15.png" alt="pic15"><br>同样地, 下图是当$y=0$时的情况<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_05/pic16.png" alt="pic16"><br>这样对代价函数处理之后, 我们的代价函数就是一个凸函数, 可以使用梯度下降来站到一个全局最优解。</p><h4 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h4><p>因为y的值只有0或1两种情况, 我们现在将$Cost\left(h_\theta(x),y\right)$用一个式子来表达:<br>$${Cost\left(h_\theta(x),y\right)=-ylog(h_\theta(x))-(1-y)log(1-h_\theta(x))}$$<br>(可以带入验证)<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_05/pic17.png" alt="pic17"><br>于是得到代价函数:<br>$${J(\theta)=-\frac{1}{m}\left[\sum_{i=1}^my^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))\right]}$$<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_05/pic18.png" alt="pic18"><br>到这里, 后面的步骤就和线性回归很相似了. 直接利用梯度下降即可. 同样地我们需要求偏导, 求完偏导之后我们发现得到的更新规则和之前线性回归的更性规则是一模一样的. 除了假设函数$h_\theta(x)$一样. 这里如果特征之间的数量级差别较大也是需要特征缩放的. (注:下图公式中$\alpha$后少了一个$1/m$)<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_05/pic19.png" alt="pic19"></p><h4 id="高级优化算法"><a href="#高级优化算法" class="headerlink" title="高级优化算法"></a>高级优化算法</h4><p>除了梯度下降还有其他更加高级更加复杂的算法：Conjugate Gradient、BFGS和L-BFGS. 如下图, 右下角是这些算法的优点和缺点。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_05/pic20.png" alt="pic20"><br>下图是一个具体的例子, 右边是自己定义的代价函数, 下方的options是调用fminunc所需的参数, initialTheta是初始的$\theta$，然后调用fminunc并传入相应的参数就可以得到最优的$\theta$。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_05/pic21.png" alt="pic21"><br>代价函数的代码如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">function [jVal, gradient] = costFunction(theta)</span><br><span class="line">jVal = (theta(1)-5)^2 + (theta(2)-5)^2;</span><br><span class="line">gradient = zeros(2,1);</span><br><span class="line">gradient(1) = 2*(theta(1)-5);</span><br><span class="line">gradient(2) = 2*(theta(2)-5);</span><br></pre></td></tr></table></figure></p><p>在Octave中演示如下:<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_05/pic22.png" alt="pic22"></p><h4 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h4><p>下图举了一些多分类(Multiclass Classification)的例子<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_05/pic23.png" alt="pic23"><br>在之前的二分类(Binary Classification)的问题中, 我们的数据集大概是如下图左侧所示. 而现在的多分类(Multi-class Classification)的问题中数据集如下图左侧所示:<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_05/pic24.png" alt="pic24"><br>我们可以使用一对多(One-vs-all/One-vs-rest)方法来处理这个问题, 即将其分成三个二分类的问题. 如下图所示:<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_05/pic25.png" alt="pic25"><br>预测时, 需要计算出$h_\theta^{(1)}(x)$、$h_\theta^{(2)}(x)$、$h_\theta^{(3)}(x)$的值并得出最大值, 其对应的分类即为预测$x$的分类。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_05/pic26.png" alt="pic26"></p>]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]]}
});
&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; async src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;blockquote&gt;
&lt;p&gt;课程地址：&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/week/3&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Logistic Regression&lt;/a&gt;&lt;br&gt;课程Wiki：&lt;a href=&quot;https://share.coursera.org/wiki/index.php/ML:Logistic_Regression&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Logistic Regression&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&quot;模型展示&quot;&gt;&lt;a href=&quot;#模型展示&quot; class=&quot;headerlink&quot; title=&quot;模型展示&quot;&gt;&lt;/a&gt;模型展示&lt;/h4&gt;&lt;h5 id=&quot;从线性回归解到对数几率回归&quot;&gt;&lt;a href=&quot;#从线性回归解到对数几率回归&quot; class=&quot;headerlink&quot; title=&quot;从线性回归解到对数几率回归&quot;&gt;&lt;/a&gt;从线性回归解到对数几率回归&lt;/h5&gt;&lt;p&gt;前面的课程中提到了一些分类问题:&lt;br&gt;&lt;img src=&quot;https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_05/pic01.png&quot; alt=&quot;pic01&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="machinelearning" scheme="http://yoursite.com/tags/machinelearning/"/>
    
      <category term="math" scheme="http://yoursite.com/tags/math/"/>
    
      <category term="note" scheme="http://yoursite.com/tags/note/"/>
    
      <category term="Octave" scheme="http://yoursite.com/tags/Octave/"/>
    
  </entry>
  
  <entry>
    <title>Coursera机器学习笔记(四) - Octave教程</title>
    <link href="http://yoursite.com/machine_learning_note_04/"/>
    <id>http://yoursite.com/machine_learning_note_04/</id>
    <published>2018-09-06T15:38:08.933Z</published>
    <updated>2018-10-13T03:36:36.647Z</updated>
    
    <content type="html"><![CDATA[<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script><script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script><blockquote><p>课程地址：<a href="https://www.coursera.org/learn/machine-learning/lecture/9fHfl/basic-operations" target="_blank" rel="noopener">Octave Tutorial</a><br>课程Wiki：<a href="https://share.coursera.org/wiki/index.php/ML:Octave_Tutorial" target="_blank" rel="noopener">Octave Tutorial</a><br>参考：<a href="http://www.gnu.org/software/octave/doc/interpreter/" target="_blank" rel="noopener">Octave documentation pages</a>&emsp;&emsp;<a href="http://www.dm.unibo.it/~lenci/teaching/14/maa/octavetut.pdf" target="_blank" rel="noopener">Introduction to Octave</a></p></blockquote><h4 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h4><h5 id="四则运算"><a href="#四则运算" class="headerlink" title="四则运算"></a>四则运算</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">octave:1&gt; 5+6</span><br><span class="line">ans =  11</span><br><span class="line">octave:2&gt; 3-2</span><br><span class="line">ans =  1</span><br><span class="line">octave:3&gt; 5*8</span><br><span class="line">ans =  40</span><br><span class="line">octave:4&gt; 1/2</span><br><span class="line">ans =  0.50000</span><br></pre></td></tr></table></figure><a id="more"></a><h5 id="逻辑运算"><a href="#逻辑运算" class="headerlink" title="逻辑运算"></a>逻辑运算</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">octave:6&gt; 1 == 2</span><br><span class="line">ans = 0</span><br><span class="line">octave:7&gt; 1 ~= 2</span><br><span class="line">ans =  1</span><br><span class="line">octave:8&gt; 1 &amp;&amp; 0</span><br><span class="line">ans = 0</span><br><span class="line">octave:9&gt; 1 || 0</span><br><span class="line">ans =  1</span><br><span class="line">octave:10&gt; xor(1,0)</span><br><span class="line">ans =  1</span><br></pre></td></tr></table></figure><h5 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h5><p>更改提示符：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">octave:11&gt; PS1(&apos;&gt;&gt; &apos;)</span><br><span class="line">&gt;&gt;</span><br></pre></td></tr></table></figure></p><p>添加分号可抑制输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; a = 1</span><br><span class="line">a =  1</span><br><span class="line">&gt;&gt; a = 1;</span><br><span class="line">&gt;&gt;</span><br></pre></td></tr></table></figure></p><p>圆周率$\pi$：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; a = pi</span><br><span class="line">a =  3.1416</span><br></pre></td></tr></table></figure></p><p>常数$e$：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; e</span><br><span class="line">ans =  2.7183</span><br></pre></td></tr></table></figure></p><p>格式化输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; disp(sprintf(&apos;6 decimals: %0.6f&apos;, a))</span><br><span class="line">6 decimals: 3.141593</span><br><span class="line">&gt;&gt; disp(sprintf(&apos;6 decimals: %0.2f&apos;, a))</span><br><span class="line">6 decimals: 3.14</span><br><span class="line">&gt;&gt; format long</span><br><span class="line">&gt;&gt; a</span><br><span class="line">a =  3.14159265358979</span><br><span class="line">&gt;&gt; format short</span><br><span class="line">&gt;&gt; a</span><br><span class="line">a =  3.1416</span><br></pre></td></tr></table></figure></p><h5 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h5><p>构造一个矩阵，方式一：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; A = [1 2; 3 4; 5 6;]</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">   1   2</span><br><span class="line">   3   4</span><br><span class="line">   5   6</span><br></pre></td></tr></table></figure></p><p>构造一个矩阵，方式二：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; A = [1 2;</span><br><span class="line">&gt; 3 4;</span><br><span class="line">&gt; 5 6;</span><br><span class="line">&gt; ]</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">   1   2</span><br><span class="line">   3   4</span><br><span class="line">   5   6</span><br></pre></td></tr></table></figure></p><p>构造一个横向量：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; v = [1 2 3]</span><br><span class="line">v =</span><br><span class="line"></span><br><span class="line">   1   2   3</span><br></pre></td></tr></table></figure></p><p>构造一个列向量：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; v = [1; 2; 3]</span><br><span class="line">v =</span><br><span class="line"></span><br><span class="line">   1</span><br><span class="line">   2</span><br><span class="line">   3</span><br></pre></td></tr></table></figure></p><p>从1到2，每次递增0.1：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; v = 1:0.1:2</span><br><span class="line">v =</span><br><span class="line"></span><br><span class="line"> Columns 1 through 5:</span><br><span class="line"></span><br><span class="line">    1.0000    1.1000    1.2000    1.3000    1.4000</span><br><span class="line"></span><br><span class="line"> Columns 6 through 10:</span><br><span class="line"></span><br><span class="line">    1.5000    1.6000    1.7000    1.8000    1.9000</span><br><span class="line"></span><br><span class="line"> Column 11:</span><br><span class="line"></span><br><span class="line">    2.0000</span><br></pre></td></tr></table></figure></p><p>从1到6，每次递增1(默认)：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; v = 1:6</span><br><span class="line">v =</span><br><span class="line"></span><br><span class="line">   1   2   3   4   5   6</span><br></pre></td></tr></table></figure></p><p>所有元素均为1：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; ones(2, 3)</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   1   1   1</span><br><span class="line">   1   1   1</span><br></pre></td></tr></table></figure></p><p>每个元素乘以2：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; C = 2*ones(2, 3)</span><br><span class="line">C =</span><br><span class="line"></span><br><span class="line">   2   2   2</span><br><span class="line">   2   2   2</span><br></pre></td></tr></table></figure></p><p>高斯随机数：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; rand(3, 3)</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   0.751588   0.906707   0.081204</span><br><span class="line">   0.411613   0.457779   0.882052</span><br><span class="line">   0.622524   0.774499   0.811092</span><br></pre></td></tr></table></figure></p><p>所有元素均为0：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; w = zeros(1, 3)</span><br><span class="line">w =</span><br><span class="line"></span><br><span class="line">   0   0   0</span><br></pre></td></tr></table></figure></p><p>单位矩阵：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; I = eye(5)</span><br><span class="line">I =</span><br><span class="line"></span><br><span class="line">Diagonal Matrix</span><br><span class="line"></span><br><span class="line">   1   0   0   0   0</span><br><span class="line">   0   1   0   0   0</span><br><span class="line">   0   0   1   0   0</span><br><span class="line">   0   0   0   1   0</span><br><span class="line">   0   0   0   0   1</span><br></pre></td></tr></table></figure></p><p>读取矩阵A第三行第二列的元素：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">A =</span><br><span class="line"></span><br><span class="line">   1   2</span><br><span class="line">   3   4</span><br><span class="line">   5   6</span><br><span class="line">&gt;&gt; A(3, 2)</span><br><span class="line">ans =  6</span><br></pre></td></tr></table></figure></p><p>读取矩阵A第2列所有元素：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; A(:,2)</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   2</span><br><span class="line">   4</span><br><span class="line">   6</span><br></pre></td></tr></table></figure></p><p>读取矩阵A第2行所有元素：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; A(2,:)</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   3   4</span><br></pre></td></tr></table></figure></p><p>读取矩阵A第1行和第3行的所有元素：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; A([1 3],:)</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   1   2</span><br><span class="line">   5   6</span><br></pre></td></tr></table></figure></p><p>将A第二列替换为[10;11;12]：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; A(:,2) = [10; 11; 12]</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">    1   10</span><br><span class="line">    3   11</span><br><span class="line">    5   12</span><br></pre></td></tr></table></figure></p><p>在A的最后加上一列：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; A = [A, [100; 101; 102]]</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">     1    10   100</span><br><span class="line">     3    11   101</span><br><span class="line">     5    12   102</span><br></pre></td></tr></table></figure></p><p>将A所有的元素合并成一个列向量:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; A(:)</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">     1</span><br><span class="line">     3</span><br><span class="line">     5</span><br><span class="line">    10</span><br><span class="line">    11</span><br><span class="line">    12</span><br><span class="line">   100</span><br><span class="line">   101</span><br><span class="line">   102</span><br></pre></td></tr></table></figure></p><p>两个矩阵的合并（列合并）:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; A = [1 2; 3 4; 5 6]</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">   1   2</span><br><span class="line">   3   4</span><br><span class="line">   5   6</span><br><span class="line"></span><br><span class="line">&gt;&gt; B = [7 8; 9 10; 11 12]</span><br><span class="line">B =</span><br><span class="line"></span><br><span class="line">    7    8</span><br><span class="line">    9   10</span><br><span class="line">   11   12</span><br><span class="line"></span><br><span class="line">&gt;&gt; C = [A B]</span><br><span class="line">C =</span><br><span class="line"></span><br><span class="line">    1    2    7    8</span><br><span class="line">    3    4    9   10</span><br><span class="line">    5    6   11   12</span><br></pre></td></tr></table></figure></p><p>两个矩阵的合并（行合并）:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; D = [A;B]</span><br><span class="line">D =</span><br><span class="line"></span><br><span class="line">    1    2</span><br><span class="line">    3    4</span><br><span class="line">    5    6</span><br><span class="line">    7    8</span><br><span class="line">    9   10</span><br><span class="line">   11   12</span><br></pre></td></tr></table></figure></p><h5 id="杂项"><a href="#杂项" class="headerlink" title="杂项"></a>杂项</h5><p>构造10000个随机数，并绘制出图形(高斯分布)：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; w=randn(1,10000);</span><br><span class="line">&gt;&gt; hist(w,50)</span><br></pre></td></tr></table></figure></p><h4 id="数据的读取和存储"><a href="#数据的读取和存储" class="headerlink" title="数据的读取和存储"></a>数据的读取和存储</h4><blockquote><p>本节所用到的数据: <a href="https://raw.githubusercontent.com/tansaku/py-coursera/master/featuresX.dat" target="_blank" rel="noopener">featuresX.dat</a>, <a href="https://raw.githubusercontent.com/tansaku/py-coursera/master/priceY.dat" target="_blank" rel="noopener">priceY.dat</a></p></blockquote><h5 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">% 找到文件所在目录：</span><br><span class="line">&gt;&gt; cd Desktop/</span><br><span class="line">&gt;&gt; cd &apos;Machine Learning/&apos;</span><br><span class="line">&gt;&gt; ls</span><br><span class="line">featuresX.datpriceY.dat</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">% 方式一：</span><br><span class="line">&gt;&gt; load featuresX.dat</span><br><span class="line">&gt;&gt; load priceY.dat</span><br><span class="line">% 方式二：</span><br><span class="line">&gt;&gt; load(&apos;featuresX.dat&apos;)</span><br><span class="line">&gt;&gt; load(&apos;priceY.dat&apos;)</span><br></pre></td></tr></table></figure><p>使用who命令显示当前所有变量：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; who</span><br><span class="line">Variables in the current scope:</span><br><span class="line"></span><br><span class="line">A          a          featuresX  v          y</span><br><span class="line">C          ans        priceY     w</span><br><span class="line">I          c          sz         x</span><br></pre></td></tr></table></figure></p><p>可以看到，刚才导入的数据已经在变量<code>featuresX</code>和<code>priceY</code>中了。<br>展示数据：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; featuresX</span><br><span class="line">featuresX =</span><br><span class="line"></span><br><span class="line">   2104      3</span><br><span class="line">   1600      3</span><br><span class="line">   2400      3</span><br><span class="line">   1416      2</span><br><span class="line">   3000      4</span><br><span class="line">   1985      4</span><br><span class="line">   1534      3</span><br><span class="line">    ...        ..</span><br><span class="line"></span><br><span class="line">&gt;&gt; size(featuresX)</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   27    2</span><br><span class="line"></span><br><span class="line">&gt;&gt; priceY</span><br><span class="line">priceY =</span><br><span class="line"></span><br><span class="line">   3999</span><br><span class="line">   3299</span><br><span class="line">   3690</span><br><span class="line">   2320</span><br><span class="line">   5399</span><br><span class="line">   2999</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">&gt;&gt; size(priceY)</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   27    1</span><br></pre></td></tr></table></figure></p><p>使用如下命令用来删除某个变量：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; clear featuresX</span><br><span class="line">&gt;&gt; whos</span><br></pre></td></tr></table></figure></p><h5 id="存储数据"><a href="#存储数据" class="headerlink" title="存储数据"></a>存储数据</h5><p>假设我们现在需要取出<code>priceY</code>前十个数据，使用如下命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; v = priceY(1:10)</span><br><span class="line">v =</span><br><span class="line"></span><br><span class="line">   3999</span><br><span class="line">   3299</span><br><span class="line">   3690</span><br><span class="line">   2320</span><br><span class="line">   5399</span><br><span class="line">   2999</span><br><span class="line">   3149</span><br><span class="line">   1989</span><br><span class="line">   2120</span><br><span class="line">   2425</span><br></pre></td></tr></table></figure></p><p>该如何存储这十个数据呢？使用<code>save</code>命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; save hello.mat v</span><br><span class="line">&gt;&gt; ls</span><br><span class="line">featuresX.dathello.matpriceY.dat</span><br></pre></td></tr></table></figure></p><p>清空所有变量：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; clear</span><br><span class="line">&gt;&gt; whos</span><br><span class="line">&gt;&gt; %无任何输出</span><br></pre></td></tr></table></figure></p><p>刚才存储数据是以二进制的形式进行存储，我们也可以使用人能够读懂的形式存储。例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; load hello.mat</span><br><span class="line">&gt;&gt; whos</span><br><span class="line">Variables in the current scope:</span><br><span class="line"></span><br><span class="line">   Attr Name        Size                     Bytes  Class</span><br><span class="line">   ==== ====        ====                     =====  =====</span><br><span class="line">        v          10x1                         80  double</span><br><span class="line"></span><br><span class="line">Total is 10 elements using 80 bytes</span><br><span class="line"></span><br><span class="line">&gt;&gt; v</span><br><span class="line">v =</span><br><span class="line"></span><br><span class="line">   3999</span><br><span class="line">   3299</span><br><span class="line">   3690</span><br><span class="line">   2320</span><br><span class="line">   5399</span><br><span class="line">   2999</span><br><span class="line">   3149</span><br><span class="line">   1989</span><br><span class="line">   2120</span><br><span class="line">   2425</span><br><span class="line"></span><br><span class="line">&gt;&gt; save hello.txt v -ascii</span><br><span class="line">&gt;&gt; ls</span><br><span class="line">featuresX.dathello.mathello.txtpriceY.dat</span><br></pre></td></tr></table></figure></p><h4 id="数据计算"><a href="#数据计算" class="headerlink" title="数据计算"></a>数据计算</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">% 各种矩阵运算</span><br><span class="line">&gt;&gt; A * B</span><br><span class="line">&gt;&gt; A .* B</span><br><span class="line">&gt;&gt; A .^ 2</span><br><span class="line">&gt;&gt; 1 ./ A</span><br><span class="line">&gt;&gt; log(A)</span><br><span class="line">&gt;&gt; exp(A)</span><br><span class="line">&gt;&gt; -A</span><br></pre></td></tr></table></figure><p>A中的每个元素都加上1：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; A + ones(size(A))</span><br></pre></td></tr></table></figure></p><p>这样也可以：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; A + 1</span><br></pre></td></tr></table></figure></p><p>矩阵转置：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; A&apos;</span><br></pre></td></tr></table></figure></p><p>向量中的最大值：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; A = [1 3 0.5 10 100]</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">     1.00000     3.00000     0.50000    10.00000   100.00000</span><br><span class="line"></span><br><span class="line">&gt;&gt; [val ind] = max(A)</span><br><span class="line">val =  100</span><br><span class="line">ind =  5</span><br></pre></td></tr></table></figure></p><p>比较大小：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; A = [1 2; 3 4; 5 6]</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">   1   2</span><br><span class="line">   3   4</span><br><span class="line">   5   6</span><br><span class="line"></span><br><span class="line">&gt;&gt; A &gt; 3</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   0   0</span><br><span class="line">   0   1</span><br><span class="line">   1   1</span><br></pre></td></tr></table></figure></p><p>找出向量中特定元素：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; find(A &gt; 3)</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   3</span><br><span class="line">   5</span><br><span class="line">   6</span><br></pre></td></tr></table></figure></p><p>找出矩阵中特定元素：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; [r c] = find(A &gt;= 3)</span><br><span class="line">r =</span><br><span class="line"></span><br><span class="line">   2</span><br><span class="line">   3</span><br><span class="line">   2</span><br><span class="line">   3</span><br><span class="line"></span><br><span class="line">c =</span><br><span class="line"></span><br><span class="line">   1</span><br><span class="line">   1</span><br><span class="line">   2</span><br><span class="line">   2</span><br></pre></td></tr></table></figure></p><p>生成任意行、列、对角线和相等的矩阵：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; magic(3)</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   8   1   6</span><br><span class="line">   3   5   7</span><br><span class="line">   4   9   2</span><br></pre></td></tr></table></figure></p><p>向量所有元素的和：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; a = [1.2 2.3 4.5 6.6]</span><br><span class="line">a =</span><br><span class="line"></span><br><span class="line">   1.2000   2.3000   4.5000   6.6000</span><br><span class="line"></span><br><span class="line">&gt;&gt; sum(a)</span><br><span class="line">ans =  14.600</span><br></pre></td></tr></table></figure></p><p>向上及向下取整：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; floor(a)</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   1   2   4   6</span><br><span class="line"></span><br><span class="line">&gt;&gt; ceil(a)</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   2   3   5   7</span><br></pre></td></tr></table></figure></p><p>构造一个由A,B两个矩阵中对应位置较大的数组成的矩阵：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">A =</span><br><span class="line"></span><br><span class="line">   1   2</span><br><span class="line">   3   4</span><br><span class="line">   5   6</span><br><span class="line"></span><br><span class="line">&gt;&gt; B = [3 1; 4 6; 2 9]</span><br><span class="line">B =</span><br><span class="line"></span><br><span class="line">   3   1</span><br><span class="line">   4   6</span><br><span class="line">   2   9</span><br><span class="line"></span><br><span class="line">&gt;&gt; max(A, B)</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   3   2</span><br><span class="line">   4   6</span><br><span class="line">   5   9</span><br><span class="line"></span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">   1   2</span><br><span class="line">   3   4</span><br><span class="line">   5   6</span><br></pre></td></tr></table></figure></p><p>取出矩阵每列最大的元素：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; max(A, [], 1)</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   5   6</span><br></pre></td></tr></table></figure></p><p>取出矩阵每行最大的元素：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; max(A, [], 2)</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   2</span><br><span class="line">   4</span><br><span class="line">   6</span><br></pre></td></tr></table></figure></p><p>想要直接获得矩阵中最大的元素，以下两种方式都可以：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">% 方式一：</span><br><span class="line">&gt;&gt; max(max(A))</span><br><span class="line">ans =  6</span><br><span class="line">% 方式二：</span><br><span class="line">&gt;&gt; max(A(:))</span><br><span class="line">ans =  6</span><br></pre></td></tr></table></figure></p><p>矩阵的上下翻转：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; eye(3)</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">Diagonal Matrix</span><br><span class="line"></span><br><span class="line">   1   0   0</span><br><span class="line">   0   1   0</span><br><span class="line">   0   0   1</span><br><span class="line"></span><br><span class="line">&gt;&gt; flipud(eye(3))</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">Permutation Matrix</span><br><span class="line"></span><br><span class="line">   0   0   1</span><br><span class="line">   0   1   0</span><br><span class="line">   1   0   0</span><br></pre></td></tr></table></figure></p><p>矩阵的逆：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; A = rand(3, 3)</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">   0.68934   0.12881   0.80507</span><br><span class="line">   0.49777   0.41907   0.37271</span><br><span class="line">   0.32607   0.27877   0.41814</span><br><span class="line"></span><br><span class="line">&gt;&gt; tmp = pinv(A)</span><br><span class="line">tmp =</span><br><span class="line"></span><br><span class="line">   1.795801   4.294380  -7.285421</span><br><span class="line">  -2.180466   0.647802   3.620828</span><br><span class="line">   0.053345  -3.780710   5.658801</span><br><span class="line"></span><br><span class="line">&gt;&gt; tmp * A</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   1.00000   0.00000   0.00000</span><br><span class="line">   0.00000   1.00000   0.00000</span><br><span class="line">  -0.00000  -0.00000   1.00000</span><br></pre></td></tr></table></figure></p><h4 id="绘制数据"><a href="#绘制数据" class="headerlink" title="绘制数据"></a>绘制数据</h4><p>绘制出sin函数图像：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = [0: 0.01: 0.98];</span><br><span class="line">&gt;&gt; y = sin(2*pi*4*x);</span><br><span class="line">&gt;&gt; plot(x,y);</span><br></pre></td></tr></table></figure></p><p>绘制出cos函数图像:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y2 = cos(2*pi*4*x);</span><br></pre></td></tr></table></figure></p><p>将两个函数绘制在一起：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; plot(x,y);</span><br><span class="line">&gt;&gt; hold on;</span><br><span class="line">&gt;&gt; plot(x,y2,&apos;r&apos;)</span><br></pre></td></tr></table></figure></p><p>添加说明：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; xlabel(&quot;time&quot;);</span><br><span class="line">&gt;&gt; ylabel(&quot;value&quot;);</span><br><span class="line">&gt;&gt; lengend(&quot;sin&quot;, &quot;cos&quot;);</span><br><span class="line">error: &apos;lengend&apos; undefined near line 1 column 1</span><br><span class="line">&gt;&gt; legend(&quot;sin&quot;, &quot;cos&quot;);</span><br></pre></td></tr></table></figure></p><p>存储图像：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; print -dpng &quot;myPlot.png&quot;</span><br></pre></td></tr></table></figure></p><p>关掉绘制的图像：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; close</span><br></pre></td></tr></table></figure></p><p>分别在两个窗口显示两个图像：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; figure(1); plot(x, y);</span><br><span class="line">&gt;&gt; figure(2); plot(x, y2);</span><br></pre></td></tr></table></figure></p><p>改变左边图像的横坐标的刻度：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; subplot(1,2,1)</span><br><span class="line">&gt;&gt; axis([0 0.5 -1 1])</span><br></pre></td></tr></table></figure></p><p>清除所有绘制的图像：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; clf</span><br></pre></td></tr></table></figure></p><p>将矩阵可视化：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; imagesc(magic(15))</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; imagesc(A), colorbar, colormap gray</span><br></pre></td></tr></table></figure><h4 id="控制语句"><a href="#控制语句" class="headerlink" title="控制语句"></a>控制语句</h4><p>for循环：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; for i=1:10,</span><br><span class="line">&gt;      v(i) = i^2;</span><br><span class="line">&gt;  end;</span><br><span class="line">&gt;&gt; v</span><br><span class="line">v =</span><br><span class="line"></span><br><span class="line">     1     4     9    16    25    36    49    64    81   100</span><br></pre></td></tr></table></figure></p><p>while循环：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; i = 1;</span><br><span class="line">&gt;&gt; while i &lt;= 10,</span><br><span class="line">&gt;      v(i) = sqrt(v(i));</span><br><span class="line">&gt;</span><br><span class="line">Display all 1753 possibilities? (y or n)</span><br><span class="line">&gt;      i = i + 1;</span><br><span class="line">&gt;  end;</span><br><span class="line">&gt;&gt; v</span><br><span class="line">v =</span><br><span class="line"></span><br><span class="line">    1    2    3    4    5    6    7    8    9   10</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; ls</span><br><span class="line">featuresX.datmyPlot.pngsquareThisNumber.m</span><br><span class="line">hello.matoctave-workspace</span><br><span class="line">hello.txtpriceY.dat</span><br><span class="line">&gt;&gt; squareThisNumber(3)</span><br><span class="line">ans =  9</span><br></pre></td></tr></table></figure><p>如果该定义的函数不在当前目录下，我们就不能使用它：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; cd ~</span><br><span class="line">&gt;&gt; pwd</span><br><span class="line">ans = /Users/bobo</span><br><span class="line">&gt;&gt; squareThisNumber(3)</span><br><span class="line">error: &apos;squareThisNumber&apos; undefined near line 1 column 1</span><br></pre></td></tr></table></figure></p><p>不过我们也可以更改Octave的搜索路径：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; addpath(&quot;~/Desktop/Machine-Learning&quot;)</span><br></pre></td></tr></table></figure></p><p>更改之后，我们现在虽然在/User/bobo目录下，但是仍然可以使用squareThisNumber函数:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; pwd</span><br><span class="line">ans = /Users/bobo</span><br><span class="line">&gt;&gt; squareThisNumber(5)</span><br><span class="line">ans =  25</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; [y1, y2] = squareAndCube(3)</span><br><span class="line">y1 =  9</span><br><span class="line">y2 =  27</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; X = [1 1; 1 2; 1 3]</span><br><span class="line">X =</span><br><span class="line"></span><br><span class="line">   1   1</span><br><span class="line">   1   2</span><br><span class="line">   1   3</span><br><span class="line"></span><br><span class="line">&gt;&gt; y = [1; 2; 3]</span><br><span class="line">y =</span><br><span class="line"></span><br><span class="line">   1</span><br><span class="line">   2</span><br><span class="line">   3</span><br><span class="line"></span><br><span class="line">&gt;&gt; theta = [0; 1]</span><br><span class="line">theta =</span><br><span class="line"></span><br><span class="line">   0</span><br><span class="line">   1</span><br><span class="line"></span><br><span class="line">&gt;&gt; costFunctionJ(X, y, theta)</span><br><span class="line">ans = 0</span><br></pre></td></tr></table></figure><p>如果$\theta=[0; 0]$<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; theta = [0; 0]</span><br><span class="line">theta =</span><br><span class="line"></span><br><span class="line">   0</span><br><span class="line">   0</span><br><span class="line"></span><br><span class="line">&gt;&gt; costFunctionJ(X, y, theta)</span><br><span class="line">ans =  2.3333</span><br></pre></td></tr></table></figure></p><h4 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h4><p><a href="https://www.coursera.org/learn/machine-learning/lecture/WnQWH/vectorization" target="_blank" rel="noopener">点击这里–&gt;</a></p>]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]]}
});
&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; async src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;blockquote&gt;
&lt;p&gt;课程地址：&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/lecture/9fHfl/basic-operations&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Octave Tutorial&lt;/a&gt;&lt;br&gt;课程Wiki：&lt;a href=&quot;https://share.coursera.org/wiki/index.php/ML:Octave_Tutorial&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Octave Tutorial&lt;/a&gt;&lt;br&gt;参考：&lt;a href=&quot;http://www.gnu.org/software/octave/doc/interpreter/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Octave documentation pages&lt;/a&gt;&amp;emsp;&amp;emsp;&lt;a href=&quot;http://www.dm.unibo.it/~lenci/teaching/14/maa/octavetut.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Introduction to Octave&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&quot;基本操作&quot;&gt;&lt;a href=&quot;#基本操作&quot; class=&quot;headerlink&quot; title=&quot;基本操作&quot;&gt;&lt;/a&gt;基本操作&lt;/h4&gt;&lt;h5 id=&quot;四则运算&quot;&gt;&lt;a href=&quot;#四则运算&quot; class=&quot;headerlink&quot; title=&quot;四则运算&quot;&gt;&lt;/a&gt;四则运算&lt;/h5&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;octave:1&amp;gt; 5+6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;ans =  11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;octave:2&amp;gt; 3-2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;ans =  1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;octave:3&amp;gt; 5*8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;ans =  40&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;octave:4&amp;gt; 1/2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;ans =  0.50000&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="machinelearning" scheme="http://yoursite.com/tags/machinelearning/"/>
    
      <category term="math" scheme="http://yoursite.com/tags/math/"/>
    
      <category term="note" scheme="http://yoursite.com/tags/note/"/>
    
      <category term="Octave" scheme="http://yoursite.com/tags/Octave/"/>
    
  </entry>
  
  <entry>
    <title>Coursera机器学习笔记(三) - 多变量线性回归</title>
    <link href="http://yoursite.com/machine_learning_note_03/"/>
    <id>http://yoursite.com/machine_learning_note_03/</id>
    <published>2018-09-06T07:04:35.958Z</published>
    <updated>2018-10-13T17:58:04.807Z</updated>
    
    <content type="html"><![CDATA[<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script><script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script><blockquote><p>课程地址：<a href="https://www.coursera.org/learn/machine-learning/lecture/6Nj1q/multiple-features" target="_blank" rel="noopener">Linear Regression with Multiple Variables</a><br>课程Wiki：<a href="https://share.coursera.org/wiki/index.php/ML:Linear_Regression_with_Multiple_Variables" target="_blank" rel="noopener">Linear Regression with Multiple Variables</a></p></blockquote><h4 id="假设函数-梯度下降"><a href="#假设函数-梯度下降" class="headerlink" title="假设函数 梯度下降"></a>假设函数 梯度下降</h4><h5 id="假设函数"><a href="#假设函数" class="headerlink" title="假设函数"></a>假设函数</h5><p>在之前的单变量线性回归中, 我们的问题只涉及到了房子面积这一个特征:<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_03/pic01.png" alt="pic01"></p><a id="more"></a><p>在实际问题中, 会有很多特征. 例如, 除了房子面积, 还有房子的卧室数量$x_2$，房子的楼层数$x_3$，房子建筑年龄$x_4$，其中$n$表示特征的数量，$m$表示训练样例的数量，$x^{(i)}$表示第$i$个训练样例，$x_j^{(i)}$表示第$i$个训练样例的第$j$个特征。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_03/pic02.png" alt="pic02"><br>在单变量线性回归中假设函数为:<br>$${h_\theta(x)=\theta_0+\theta_1x}$$<br>类似地, 现在假设函数记作：<br>$${h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n}$$<br>可是每次这样写太麻烦了, 为了方便首先定义$x_0=1$（即$x_0^{(i)}=1$），此时$x_0^{(i)}=1$为：<br>$${h_\theta(x)=\theta_0x_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n}$$<br>再令${\qquad\qquad\theta=\begin{bmatrix}\theta_0\ \theta_1\ \theta_2\.\.\.\ \theta_n \end{bmatrix}\in \rm I!R^{n+1}\quad,\qquad\qquad}$ ${x=\begin{bmatrix}x_0\x_1\x_2\.\.\.\x_n \end{bmatrix}\in \rm I!R^{n+1}}$<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_03/pic03.png" alt="pic03"><br>这样就得到了假设函数的向量表示:<br>$${h_\theta(x)=\theta_0x_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n= \theta^Tx}$$</p><h5 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h5><p>多变量情况下的梯度下降其实没有区别, 只需要把对应的偏导数项换掉即可。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_03/pic04.png" alt="pic04"></p><h4 id="特征处理"><a href="#特征处理" class="headerlink" title="特征处理"></a>特征处理</h4><h5 id="特征缩放"><a href="#特征缩放" class="headerlink" title="特征缩放"></a>特征缩放</h5><p>如果每个特征的范围相差的很大, 梯度下降会很慢. 为了解决这个问题, 我们在梯度下降之前应该对数据做特征归缩放(Feature Scaling)处理，从而将所有的特征的数量级都在一个差不多的范围之内, 以加快梯度下降的速度。<br>假设现在我们有两个特征, 房子的面积和房间的数量. 如下图所示, 他们的范围相差的非常大. 对于这样的数据, 它的代价函数大概如下图左边, 梯度下降要经过很多很多次的迭代才能达到最优点. 如果我们对这两个特征按照右边给出的公式进行特征缩放, 那么此时的代价函数如下图右边所示, 相对于之前, 可以大大减少梯度下降的迭代次数。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_03/pic05.png" alt="pic05"><br>通常我们需要把特征都缩放到$[-1,1]$（附近）这个范围。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_03/pic06.png" alt="pic06"></p><h5 id="均值归一化"><a href="#均值归一化" class="headerlink" title="均值归一化"></a>均值归一化</h5><p>还有一个特征处理的方法就是均值归一化(Mean normalization):<br>$${x_i=\frac{x_i-\mu_i}{max-min}}$$<br>或者<br>$${x_i=\frac{x_i-\mu_i}{\sigma_i}}$$<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_03/pic07.png" alt="pic07"></p><h4 id="代价函数和学习率"><a href="#代价函数和学习率" class="headerlink" title="代价函数和学习率"></a>代价函数和学习率</h4><p>我们可以通过画出$\mathop{min}\limits_{\theta}J(\theta)$与迭代次数数的关系图来观察梯度下降的运行. 如下图所示, 横坐标是迭代次数, 纵坐标是代价函数的值. 如果梯度算法正常运行的话, 代价函数的图像大概的形状如下图所示。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_03/pic08.png" alt="pic08"><br>还有一种叫自动收敛测试的方法, 即每次迭代之后观察$J(\theta)$，的值, 如果迭代之后下降的值小于$\epsilon$（例如$\epsilon=10^{-3}$），就判定为收敛. 不过准确地选择阈值$\epsilon$是非常困难的，通常还是使用画图的方法。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_03/pic09.png" alt="pic09"><br>如果出现了下面的两种情况, 这个时候应该选择更小的$\alpha$，注意: 1.$\alpha$足够小, 那么$J(\theta)$在每次迭代之后都会减小；2.但是如果太小, 梯度下降会进行的非常缓慢。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_03/pic10.png" alt="pic10"><br>可以使用下面几个值进行尝试<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_03/pic11.png" alt="pic11"></p><h4 id="特征选择和多项式回归"><a href="#特征选择和多项式回归" class="headerlink" title="特征选择和多项式回归"></a>特征选择和多项式回归</h4><p>假设预测房屋价格, 选取房屋的长和宽作为变量, 得到如下的假设函数：<br>$$h(\theta)=\theta_0+\theta_1\times frontage+\theta_1\times depth$$<br>当然, 我们觉得真正决定房屋价格应该是与房屋的面积有关. 这时候我们也可以重新选择我们的特征$x=frontage\times depth$，此时的假设函数为：<br>$$h(\theta)=\theta_0+\theta_1x$$<br>通过这种特征的选择, 我们可能得到一个更好的模型<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_03/pic12.png" alt="pic12"><br>和这个密切相关的一个概念就是多项式回归(Polynomial Regression). 假设有下图所示的关于房屋价格的数据集, 我们有多种模型去拟合(下图右所示). 第一个模型是一个二次函数, 但是二次函数是一个抛物线, 这里不符合(因为房价不会随着房子面积的增加二减小)；所以我们选择三次函数的模型, 想要使用该模型去拟合. 那么我们该如何将这个模型运用在我们的数据上呢？我们可以将房屋的面积作为第一个特征, 面积的平方作为第二个特征, 面积的立方作为第三个特征, 如下图左下角所示. (这里需要注意的是, $x_0,x_1,x_2$的范围差别会非常大, 所以一定要进行特征缩放处理）<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_03/pic13.png" alt="pic13"><br>除了三次函数模型, 这里也可以选择平方根函数模型, 如下图所示<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_03/pic14.png" alt="pic14"></p><h4 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h4><h5 id="正规方程-1"><a href="#正规方程-1" class="headerlink" title="正规方程"></a>正规方程</h5><p>之前我们一直是用的梯度下降求解最优值. 它的缺点就是需要进行很多次迭代才能得到全局最优解. 有没有更好的方法呢? 我们先来看一个最简单的例子, 假设现在的代价函数为$J(\theta)=a\theta^2+b\theta+c$，$\theta$是一个实数. 怎样得到最优解? 很简单, 只要令它的导数为0就可以了。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_03/pic15.png" alt="pic15"><br>事实上, 代价函数不会像例子那样简单, $\theta$也不是一个实数，而是一个$n+1$维的向量，这样, 我们分别对每个$\theta$求偏导，再令偏导等于0, 可以计算出左右的$\theta$，了. 但看上去还是很繁琐, 所以下面我们介绍一种向量化的求解方法。<br>首先, 在数据集前加上一列$x_0$，值都为1；然后将所有的变量都放入矩阵$X$中(包括加上的$x_0$)；再将输出值放入向量$y$中。最后通过公式$\theta=(X^TX)^{-1}X^Ty$，就可以算出$\theta$的值。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_03/pic16.png" alt="pic16"><br>下图是一个更通用的表达方式：<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_03/pic17.png" alt="pic17"><br>在Octave中, 可用如下命令计算:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pinv(x&apos;*x)*x&apos;*y</span><br></pre></td></tr></table></figure></p><p>这个公式叫做正规方程, 使用这种方法还有一个好处就是不需要进行特征缩放处理。</p><h5 id="梯度下降与正规方程的比较"><a href="#梯度下降与正规方程的比较" class="headerlink" title="梯度下降与正规方程的比较"></a>梯度下降与正规方程的比较</h5><p>下图是梯度下降(Gradient Descent)和正规方程(Normal Equation)两种方法优缺点的比较：<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_03/pic18.png" alt="pic18"></p><h5 id="正规方程不可逆的情况"><a href="#正规方程不可逆的情况" class="headerlink" title="正规方程不可逆的情况"></a>正规方程不可逆的情况</h5><p>使用正规方程还有一个问题就是$X^TX$可能存在不可逆的情况。这个时候, 可能是因为我们使用了冗余的特征, 还有一个原因是我们使用了太多的特征(特征的数量超过了样本的数量)。对于这种情况我们可以删掉一些特征或者使用正则化(正则化在后面的课中讲)。</p>]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]]}
});
&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; async src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;blockquote&gt;
&lt;p&gt;课程地址：&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/lecture/6Nj1q/multiple-features&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Linear Regression with Multiple Variables&lt;/a&gt;&lt;br&gt;课程Wiki：&lt;a href=&quot;https://share.coursera.org/wiki/index.php/ML:Linear_Regression_with_Multiple_Variables&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Linear Regression with Multiple Variables&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&quot;假设函数-梯度下降&quot;&gt;&lt;a href=&quot;#假设函数-梯度下降&quot; class=&quot;headerlink&quot; title=&quot;假设函数 梯度下降&quot;&gt;&lt;/a&gt;假设函数 梯度下降&lt;/h4&gt;&lt;h5 id=&quot;假设函数&quot;&gt;&lt;a href=&quot;#假设函数&quot; class=&quot;headerlink&quot; title=&quot;假设函数&quot;&gt;&lt;/a&gt;假设函数&lt;/h5&gt;&lt;p&gt;在之前的单变量线性回归中, 我们的问题只涉及到了房子面积这一个特征:&lt;br&gt;&lt;img src=&quot;https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_03/pic01.png&quot; alt=&quot;pic01&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="machinelearning" scheme="http://yoursite.com/tags/machinelearning/"/>
    
      <category term="math" scheme="http://yoursite.com/tags/math/"/>
    
      <category term="note" scheme="http://yoursite.com/tags/note/"/>
    
  </entry>
  
  <entry>
    <title>《极恶非道》观后感</title>
    <link href="http://yoursite.com/jiefeidao_comment/"/>
    <id>http://yoursite.com/jiefeidao_comment/</id>
    <published>2018-09-05T14:18:06.603Z</published>
    <updated>2018-09-05T14:18:06.603Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>无关乎生存，仅在乎利益。这是电影交代的黑帮从上而下的恶，这种恶充斥了整部电影，不仅黑帮，也有警察。</p></blockquote><p><img src="https://pic2.zhimg.com/v2-3dec37c2122bb378247032a377f957d4_qhd.jpg" alt="北野武照片"></p><p>北野武是以拍摄黑帮片出道，早期的《奏鸣曲》是一部故事完全发生黑帮内部的电影，但北野武没有把电影拍成类型化的模式，而是嚼碎了各种类型化的元素，为之注入了他个人独特的对于荒诞生命的严峻思考。2000年赴美拍摄的《大佬》倒是比较类型化，影片不断展示帮派战斗的过程，还充满了东西方文化的差异比较，生死相托的日式侠义精神在影片中被反复讴歌。<br><a id="more"></a><br>《极恶非道》是北野武十年后回归暴力黑社会的作品。影片的叙事法则便是你来我往的夺权战斗。叙事出发点始于关内会长（北村总一朗 扮）要抢夺村濑（石桥莲司 扮）组的地盘，用的策略是并不高明的挑拨离间，让曾经与村濑为狱友的池元（国村隼 扮）与之大战。池元手下的大友（北野武 扮）组与村濑组的木村（中野英雄 扮）也随之展开斗争。这是一部众生相电影，与之相关的重要角色还有大友组的石原（加濑亮 扮）、水野（椎名桔平 扮），会长手下的加藤（三浦友和 扮），负责暴力团事物的警察片冈。影片的日文片名直译是《全员恶人》，这里面确实没有一个道德方面的善良之辈，没有英雄，只有恶人。尤其是几个首领，关内会长、池元、村濑，没有任何道义和领袖魅力可言，有的只是利欲熏心。过往日本黑帮组织的一些情义亦是荡然无存。池元与村濑本来是狱友，在《无仁义之战》第一集中，狱友曾经是被表现为敢用性命相托的赤胆忠诚之关系，在本片中却是一文不值，面对质问，池元甚至说，“拜把只是个仪式”。木村连切手指都不敢。而当大友当真用切手指的方法道歉时，却被会长讥讽这过时了，“老套的切手指没有用”。在此特别值得一提的是北野扮演的大友这个角色，正是这个角色的存在为这部黑帮片注入了一些别样的独属于北野武式思考的活力。</p><p>大友在影片虽亦是热衷暴力的恶人，但在道义上却是偏向正面。他当真视池元为干爹，为其卖命，但随后看透了这套把戏之后，意识到这是一场你死我活的战斗。黑帮中人，本来已经看透生死，当意识到道之不存后，大友便将这场战斗看成了无关道义的死亡游戏。死亡到来的时刻便是当下澄明的解脱时刻，这是北野武式洒脱豁达当下即是的生命哲学。正是这个视点又为这部类型化程度很高的黑帮电影，注入了一种抽离的客观化的视点。也正是在北野武独特生命哲学的关照之下，与死亡密切相关的暴力，被北野武表达的如此极致。北野武多年来的采访中曾多次表达了个人对暴力的看法，他讨厌美感的暴力，他认为暴力的本质就是伤痛，“暴力就是暴力，我希望用暴力的镜头刺痛观众，让他们知道暴力有多么糟糕，所以我不会顾忌该用什么样的方式，用多野蛮的力度。也许你们看电影的时候，已经觉得承受不了了，但其实这是我故意制造出来的，我希望让观众感到剧烈的疼痛和恐惧，这种感觉就是暴力的本质。我最讨厌那种把暴力拍得很美的电影，还冠以暴力美学的美称，那样的电影才是教坏小孩子的罪魁祸首”。吴宇森式的自我崇高化的浪漫暴力，或者塔伦蒂诺式卡通恶趣味暴力，在北野武的电影中是看不到的。瞬间爆发的，极致的，带来无比伤痛的暴力才是真正的北野暴力。在《极恶非道》系列中，北野勾画出的暴力类型亦是极端残酷：电钻凿牙齿、筷子猛插耳朵、猛击舌头导致卡舌而死、棒球连续性地击砸面门。最汹涌澎湃的无情暴力莫过于与大友并肩作战到最后的水野：头颅被绳子套住后飞车拉断而死。</p><p>《极恶非道》系列第二集的主题与叙事模式与第一集类似，只是权斗的始作俑者变成了警察片冈。警方这套挑拨离间以暴易暴的制敌模式也是其来有自。1987年，一和会与山口组发生了激烈的暴力抗争。当时，一合会打算在大阪地区刺杀新上任的山口组组长竹中正久，他们用了二十多部无线电对讲机。关西电信局发现情况后报警，但警方不予理睬，为的就是帮助一合会行刺，以暴易暴。不过片冈在影片中并非正义人士，而是猥琐的敲诈犯，当大友看破这死局后，二话不说一枪击毙片冈。影片的高潮来得猝不及防，但却酣畅淋漓，激越又超然。</p><p>这是一部非常纯正的yakuza电影，而电影中表现的暴力和深作欣二的热血翻滚不同，北野武的暴力凝固至冰点，让人看完脊背发凉。</p><blockquote><p>最激烈的暴力与死亡冲动同样可以是最深邃的生命顿悟。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;无关乎生存，仅在乎利益。这是电影交代的黑帮从上而下的恶，这种恶充斥了整部电影，不仅黑帮，也有警察。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3dec37c2122bb378247032a377f957d4_qhd.jpg&quot; alt=&quot;北野武照片&quot;&gt;&lt;/p&gt;
&lt;p&gt;北野武是以拍摄黑帮片出道，早期的《奏鸣曲》是一部故事完全发生黑帮内部的电影，但北野武没有把电影拍成类型化的模式，而是嚼碎了各种类型化的元素，为之注入了他个人独特的对于荒诞生命的严峻思考。2000年赴美拍摄的《大佬》倒是比较类型化，影片不断展示帮派战斗的过程，还充满了东西方文化的差异比较，生死相托的日式侠义精神在影片中被反复讴歌。&lt;br&gt;
    
    </summary>
    
      <category term="影评" scheme="http://yoursite.com/categories/%E5%BD%B1%E8%AF%84/"/>
    
    
      <category term="北野武" scheme="http://yoursite.com/tags/%E5%8C%97%E9%87%8E%E6%AD%A6/"/>
    
      <category term="yakuza" scheme="http://yoursite.com/tags/yakuza/"/>
    
  </entry>
  
  <entry>
    <title>Coursera机器学习笔记(二) - 单变量线性回归(补充 - 梯度下降算法)</title>
    <link href="http://yoursite.com/machine_learning_note_02_addition/"/>
    <id>http://yoursite.com/machine_learning_note_02_addition/</id>
    <published>2018-09-05T13:51:57.176Z</published>
    <updated>2018-10-13T03:15:41.138Z</updated>
    
    <content type="html"><![CDATA[<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script><script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script><h4 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h4><p>梯度下降算法是一种优化算法, 它可以帮助我们找到一个函数的局部极小值点. 它不仅仅可以用在线性回归模型中, 在机器学习许多其他的模型中也可以使用它. 对于我们现在研究的单变量线性回归来说, 我们想要使用梯度下降来找到最优的$\theta_0$、$\theta_1$。它的思想是, 首先随机选择两个$\theta_0$、$\theta_1$，不断地改变它们的值使得$J(\theta_0,\theta_1)$变小，最终找到$J(\theta_0,\theta_1)$的最小值。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02_addition/pic01.png" alt="pic01"></p><a id="more"></a><p>可以把梯度下降的过程想象成下山坡, 如果想要尽可能快的下坡, 应该每次都往坡度最大的方向下山。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02_addition/pic02.png" alt="pic02"><br>梯度下降算法得到的结果会受到初始状态的影响, 即当从不同的点开始时, 可能到达不同的局部极小值, 如下图:<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02_addition/pic03.png" alt="pic03"><br>下面具体看一下算法的过程, 如下图所示, 其中$:=$表示赋值，$\alpha$表示学习率用来控制下降的幅度，$\frac{\partial}{\partial\theta_j}J(\theta_0, \theta_1)$表示梯度。这里一定要注意的是，算法每次是同时(simultaneously)改变$\theta_0$和$\theta_1$的值，如下图所示<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02_addition/pic04.png" alt="pic04"></p><h4 id="梯度和学习率"><a href="#梯度和学习率" class="headerlink" title="梯度和学习率"></a>梯度和学习率</h4><p>我们先来看看梯度下降算法的梯度是如何帮助我们找到最优解的. 为了研究问题的方便我们还是同样地令$\theta_0=0$，假设一开始选取的$\theta_1$在最低点右侧，此时的梯度(斜率)是一个正数。根据上面的算法更新$\theta_1$的时候，它的值会减小, 即靠近最低点。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02_addition/pic05.png" alt="pic05"><br>类似地假设一开始选取的$\theta_1$在最低点的左侧，此时的梯度是一个负数，根据上面的算法更新$\theta_1$的时候，它的值会增大，也会靠近最低点。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02_addition/pic06.png" alt="pic06"><br>如果一开始选取的$\theta_1$恰好在最适位置，那么更新$\theta_1$时，它的值不会发生变化。</p><p>学习率$\alpha$会影响梯度下降的幅度。如果$\alpha$太小，$\theta$的值每次会变化的很小，那么梯度下降就会非常慢；相反地，如果$\alpha$太大，$\theta$的值每次会变化的很大，有可能直接越过最低点，可能导致永远没法到达最低点。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02_addition/pic08.png" alt="pic08"><br>由于随着越来越接近最低点, 相应的梯度(绝对值)也会逐渐减小，所以每次下降程度就会越来越小, 我们并不需要减小$\alpha$的值来减小下降程度。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02_addition/pic09.png" alt="pic09"></p><h4 id="计算梯度"><a href="#计算梯度" class="headerlink" title="计算梯度"></a>计算梯度</h4><p>根据定义, 梯度也就是代价函数对每个$\theta$求偏导：<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02_addition/pic10.png" alt="pic10"><br>我们将$h_\theta(x^{(i)})=\theta_0+\theta_1x^{(i)}$带入到$J(\theta_0,\theta_1)$中，并且分别对$\theta_0$，$\theta_1$求导得：<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02_addition/pic11.png" alt="pic11"><br>由此得到了完整的梯度下降算法:<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02_addition/pic12.png" alt="pic12"><br>前面说了梯度下降算法得到的结果会受初始状态的影响, 即初始状态不同, 结果可能是不同的局部最低点。<br>事实上，用于线性回归的代价函数总是一个凸函数(Convex Function)。这样的函数没有局部最优解，只有一个全局最优解。所以我们在使用梯度下降的时候，总会得到一个全局最优解。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02_addition/pic14.png" alt="pic14"><br>下面我们来看一下梯度下降的运行过程：<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02_addition/pic15.png" alt="pic15"><br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02_addition/pic16.png" alt="pic16"><br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02_addition/pic17.png" alt="pic17"><br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02_addition/pic18.png" alt="pic18"><br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02_addition/pic19.png" alt="pic19"><br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02_addition/pic20.png" alt="pic20"><br>迭代多次后，我们得到了最优解。现在我们可以用最优解对应的假设函数来对房价进行预测了。</p>]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]]}
});
&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; async src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;h4 id=&quot;梯度下降&quot;&gt;&lt;a href=&quot;#梯度下降&quot; class=&quot;headerlink&quot; title=&quot;梯度下降&quot;&gt;&lt;/a&gt;梯度下降&lt;/h4&gt;&lt;p&gt;梯度下降算法是一种优化算法, 它可以帮助我们找到一个函数的局部极小值点. 它不仅仅可以用在线性回归模型中, 在机器学习许多其他的模型中也可以使用它. 对于我们现在研究的单变量线性回归来说, 我们想要使用梯度下降来找到最优的$\theta_0$、$\theta_1$。它的思想是, 首先随机选择两个$\theta_0$、$\theta_1$，不断地改变它们的值使得$J(\theta_0,\theta_1)$变小，最终找到$J(\theta_0,\theta_1)$的最小值。&lt;br&gt;&lt;img src=&quot;https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02_addition/pic01.png&quot; alt=&quot;pic01&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="machinelearning" scheme="http://yoursite.com/tags/machinelearning/"/>
    
      <category term="math" scheme="http://yoursite.com/tags/math/"/>
    
      <category term="note" scheme="http://yoursite.com/tags/note/"/>
    
  </entry>
  
  <entry>
    <title>Coursera机器学习笔记(二) - 单变量线性回归</title>
    <link href="http://yoursite.com/machine_learning_note_02/"/>
    <id>http://yoursite.com/machine_learning_note_02/</id>
    <published>2018-09-04T14:27:55.458Z</published>
    <updated>2018-10-12T08:52:53.708Z</updated>
    
    <content type="html"><![CDATA[<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script><script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script><blockquote><p>课程地址：<a href="https://www.coursera.org/learn/machine-learning/home/week/1" target="_blank" rel="noopener">Linear Regression with One Variable</a><br>课程Wiki：<a href="https://share.coursera.org/wiki/index.php/ML:Linear_Regression_with_One_Variable" target="_blank" rel="noopener">Linear Regression with One Variable</a></p></blockquote><h4 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h4><h5 id="训练集"><a href="#训练集" class="headerlink" title="训练集"></a>训练集</h5><p>由训练样例(training example)组成的集合就是训练集(training set)，如下图所示, 其中$(x, y)$是一个训练样例, $(x^{(i)}, y^{(i)})$第i个训练样例。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02/pic01.png" alt="pic01"></p><a id="more"></a><h5 id="假设函数"><a href="#假设函数" class="headerlink" title="假设函数"></a>假设函数</h5><p>使用某种学习算法对训练集的数据进行训练, 我们可以得到假设函数(Hypothesis Function), 如下图所示. 在房价的例子中，假设函数就是一个房价关于房子面积的函数。有了这个假设函数之后, 给定一个房子的面积我们就可以预测它的价格了。<br>我们使用如下的形式表示假设函数, 为了方便，$h_\theta(x)$也可以记作$h(x)$<br>$${h_\theta(x)=\theta_0+\theta_1x}$$<br>以上这个模型就叫做单变量的线性回归(Linear Regression with One Variable). (Linear regression with one variable = Univariate linear regression，univariate是one variable的装逼写法。)<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02/pic03.png" alt="pic03"></p><h4 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h4><h5 id="什么是代价函数"><a href="#什么是代价函数" class="headerlink" title="什么是代价函数"></a>什么是代价函数</h5><p>只要我们知道了假设函数, 我们就可以进行预测了. 关键是, 假设函数中有两个未知的量$\theta_0, \theta_1$，当选择不同的$\theta_0$和$\theta_1$时，我们模型的效果肯定是不一样的。如下图所示, 列举了三种不同的$\theta_0$和$\theta_1$下的假设函数。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02/pic04.png" alt="pic04"><br>现在的问题就是该如何选择这两个参数了。我们的想法是选择某个$\theta_0$和$\theta_1$，使得对于训练样例$(x,y)$，$h_\theta(x)$最接近$y$。越是接近, 代表这个假设函数越是准确, 这里我们选择均方误差来作为衡量标准, 即我们想要每个样例的估计值与真实值之间差的平方的均值最小。用公式表达为:<br>$${\mathop{minimize}\limits_{\theta_0,\theta_1} \frac{1}{2m}\sum_{i=0}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)^2}$$<br>（其中$1/2$是为了后面方便计算）我们记<br>$${J(\theta_0,\theta_1)=\frac{1}{2m}\sum_{i=0}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)^2}$$<br>这样就得到了我们的代价函数(cost function), 也就是我们的优化目标, 我们想要代价函数最小:<br>$$\mathop{minimize}\limits_{\theta_0,\theta_1}J(\theta_0,\theta_1)$$<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02/pic05.png" alt="pic05"></p><h5 id="代价函数和假设函数"><a href="#代价函数和假设函数" class="headerlink" title="代价函数和假设函数"></a>代价函数和假设函数</h5><p>现在为了更方便地探究$h_\theta(x)$和$J(\theta_0,\theta_1)$的关系，先令$\theta_0=0$，得到了简化后的假设函数，有假设函数的定义可知此时的假设函数是经过原点的直线. 相应地也也得到简化的代价函数。如图所示:<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02/pic06.png" alt="pic06"><br>简化之后，我们令$\theta_1=1$，就得到$h_\theta(x)=x$，如下图左所示。图中三个红叉表示训练样例，通过代价函数的定义我们计算得出$J(1)=0$，对应下图右中的$(0,1)$坐标<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02/pic07.png" alt="pic07"><br>重复上面的步骤，再令$\theta_1=0.5$，得到$h_\theta(x)$如下图左所示。通过计算得出$J(0.5)=0.58$，对应下图右中的$(0.5,0.58)$<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02/pic08.png" alt="pic08"><br>对于不同的$\theta_1$，对应着不同的假设函数$h_\theta(x)$，于是就有了不同的$J(\theta_1)$。将这些点连接起来就可以得到$J(\theta_1)$关于$\theta_1$的函数图像，如下图所示：<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02/pic09.png" alt="pic09"><br>我们的目标就是找到一个$\theta$使得$J(\theta)$最小，通过上面的描点作图的方式, 我们可以从图中看出, 当$\theta=1$的时候，$J(\theta)$取到最小值。</p><h5 id="代价函数与假设函数II"><a href="#代价函数与假设函数II" class="headerlink" title="代价函数与假设函数II"></a>代价函数与假设函数II</h5><p>在上一节中，我们令$\theta_0=0$，并且通过设置不同的$\theta_1$来描点作图得到$J(\theta_1)$的曲线。这一节我们不再令$\theta_0=0$，而是同时设置$\theta_0$和$\theta_1$的值，然后再绘出$J(\theta_0, \theta_1)$的图形。因为此时有两个变量，很容易想到$J(\theta_0, \theta_1)$应该是一个曲面, 如下图所示:<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02/pic10.png" alt="pic10"><br>这个图是教授用matlab绘制的，由于3D图形不太方便我们研究，我们就使用二维的等高线(上图右上角教授写的contour plots/figures)，这样看上去比较清楚一些。如下图右，越靠近中心表示$J(\theta_0, \theta_1)$的值越小(对应3D图中越靠近最低点的位置)。下图左表示当$\theta_0=800$，$\theta_1=0.15$的时候对应的$h_\theta(x)$，通过$\theta_0$和$\theta_1$的值可以找到下图右中$J(\theta_0, \theta_1)$的值。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02/pic11.png" alt="pic11"><br>类似地：<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02/pic12.png" alt="pic12"><br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02/pic13.png" alt="pic13"><br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02/pic14.png" alt="pic14"><br>我们不断尝试直到找到一个最佳的$h_\theta(x)$。是否有特定的算法能帮助我们找到最佳的$h_\theta(x)$呢？下面我们就要介绍这个算法-<a href="https://bigmoutain.cn/machine_learning_note_02_addition/" target="_blank" rel="noopener">梯度下降算法</a>。</p>]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]]}
});
&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; async src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;


&lt;blockquote&gt;
&lt;p&gt;课程地址：&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/week/1&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Linear Regression with One Variable&lt;/a&gt;&lt;br&gt;课程Wiki：&lt;a href=&quot;https://share.coursera.org/wiki/index.php/ML:Linear_Regression_with_One_Variable&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Linear Regression with One Variable&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&quot;基本概念&quot;&gt;&lt;a href=&quot;#基本概念&quot; class=&quot;headerlink&quot; title=&quot;基本概念&quot;&gt;&lt;/a&gt;基本概念&lt;/h4&gt;&lt;h5 id=&quot;训练集&quot;&gt;&lt;a href=&quot;#训练集&quot; class=&quot;headerlink&quot; title=&quot;训练集&quot;&gt;&lt;/a&gt;训练集&lt;/h5&gt;&lt;p&gt;由训练样例(training example)组成的集合就是训练集(training set)，如下图所示, 其中$(x, y)$是一个训练样例, $(x^{(i)}, y^{(i)})$第i个训练样例。&lt;br&gt;&lt;img src=&quot;https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_02/pic01.png&quot; alt=&quot;pic01&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="machinelearning" scheme="http://yoursite.com/tags/machinelearning/"/>
    
      <category term="math" scheme="http://yoursite.com/tags/math/"/>
    
      <category term="note" scheme="http://yoursite.com/tags/note/"/>
    
  </entry>
  
  <entry>
    <title>Coursera机器学习笔记(一) - 监督学习vs无监督学习</title>
    <link href="http://yoursite.com/machine_learning_note_01/"/>
    <id>http://yoursite.com/machine_learning_note_01/</id>
    <published>2018-09-04T14:03:23.953Z</published>
    <updated>2018-10-12T05:43:02.609Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>课程地址：<a href="https://www.coursera.org/learn/machine-learning/lecture/1VkCb/supervised-learning" target="_blank" rel="noopener">Supervised Learning &amp; Unsupervised Learning</a><br>课程Wiki：<a href="https://share.coursera.org/wiki/index.php/ML:Introduction" target="_blank" rel="noopener">Introduction</a></p></blockquote><h4 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h4><p>什么是监督学习? 我们来看看维基百科中给出的定义:</p><blockquote><p>监督式学习（英语：Supervised learning），是一个机器学习中的方法，可以由训练资料中学到或建立一个模式（函数 / learning model），并依此模式推测新的实例。训练资料是由输入物件（通常是向量）和预期输出所组成。函数的输出可以是一个连续的值（称为回归分析），或是预测一个分类标签（称作分类）</p></blockquote><p>从数据的角度来讲, 监督学习和无监督学习的区别就在于监督学习的数据不仅仅有特征组成, 即每一个数据样本都包含一个准确的输出值. 在房价预测的问题中, 数据由特征+房价组成。</p><a id="more"></a><h5 id="监督学习的分类"><a href="#监督学习的分类" class="headerlink" title="监督学习的分类"></a>监督学习的分类</h5><p>在监督学习中, 我们的预测结果可以是连续值, 也可以是离散值。我们根据这样的属性将监督学习分为回归问题和分类问题。<br>下面我们分别举一个例子来看看, 学完这两个例子之后, 我们就会对监督学习, 回归以及分类有比较清晰地认识了。</p><h5 id="监督学习举例"><a href="#监督学习举例" class="headerlink" title="监督学习举例"></a>监督学习举例</h5><h6 id="回归问题"><a href="#回归问题" class="headerlink" title="回归问题"></a>回归问题</h6><p>我们现在有这么一个问题, 我们想通过给定的一个房子的面积来预测这个房子在市场中的价格。这里的房子的面积就是特征, 房子的价格就是一个输出值。为了解决这个问题, 我们获取了大量的房地产数据, 每一条数据都包含房子的面积及其对应价格. 第一, 我们的数据不仅包含房屋的面积, 还包含其对应的价格, 而我们的目标就是通过面积预测房价。所以这应该是一个监督学习; 其次, 我们的输出数据房价可以看做是连续的值, 所以这个问题是一个回归问题。至于如何通过数据得到可以使用的模型, 后面的几节课再做讨论。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_01/pic01.png" alt="pic01"></p><h6 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h6><p>再来看一个分类问题, 从名字上来讲, 分类问题还是比较好理解的, 我们的目标应该是要对数据进行分类。现在我们的数据是有关乳腺癌的医学数据, 它包含了肿瘤的大小以及该肿瘤是良性的还是恶性的。我们的目标是给定一个肿瘤的大小来预测它是良性还是恶性. 我们可以用0代表良性，1代表恶性。这就是一个分类问题, 因为我们要预测的是一个离散值。当然, 在这个例子中, 我们的离散值可以去’良性’或者’恶性’. 在其他分类问题中, 离散值可能会大于两个。例如在该例子中可以有{0,1,2,3}四种输出，分别对应{良性, 第一类肿瘤, 第二类肿瘤, 第三类肿瘤}。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_01/pic02.png" alt="pic02"><br>在这个例子中特征只有一个即瘤的大小。 对于大多数机器学习的问题, 特征往往有多个(上面的房价问题也是, 实际中特征不止是房子的面积)..例如下图， 有“年龄”和“肿瘤大小”两个特征。(还可以有其他许多特征，如下图右侧所示)<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_01/pic03.png" alt="pic03"></p><h4 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h4><p>在监督学习中我们也提到了它与无监督学习的区别。在无监督学习中, 我们的数据并没有给出特定的标签, 例如上面例子中的房价或者是良性还是恶性。我们目标也从预测某个值或者某个分类便成了寻找数据集中特殊的或者对我们来说有价值结构。<br>我们也可以从图中看到, 大概可以将数据及分成两个簇。将数据集分成不同簇的无监督学习算法也被称为聚类算法。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_01/pic05.png" alt="pic05"></p><h5 id="无监督学习举例"><a href="#无监督学习举例" class="headerlink" title="无监督学习举例"></a>无监督学习举例</h5><h6 id="新闻分类"><a href="#新闻分类" class="headerlink" title="新闻分类"></a>新闻分类</h6><p>第一个例子举的是Google News的例子。Google News搜集网上的新闻，并且根据新闻的主题将新闻分成许多簇, 然后将在同一个簇的新闻放在一起。如图中红圈部分都是关于BP Oil Well各种新闻的链接，当打开各个新闻链接的时候，展现的都是关于BP Oil Well的新闻。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_01/pic06.png" alt="pic06"></p><h6 id="根据给定基因将人群分类"><a href="#根据给定基因将人群分类" class="headerlink" title="根据给定基因将人群分类"></a>根据给定基因将人群分类</h6><p>如图是DNA数据，对于一组不同的人我们测量他们DNA中对于一个特定基因的表达程度。然后根据测量结果可以用聚类算法将他们分成不同的类型。<br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_01/pic07.png" alt="pic07"></p><h6 id="鸡尾酒排队效应"><a href="#鸡尾酒排队效应" class="headerlink" title="鸡尾酒排队效应"></a>鸡尾酒排队效应</h6><p>详见课程: <a href="https://www.coursera.org/learn/machine-learning/lecture/olRZo/unsupervised-learning" target="_blank" rel="noopener">Unsupervised Learning</a></p><h6 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h6><p>这里又举了其他几个例子，有组织计算机集群，社交网络分析，市场划分，天文数据分析等。具体可以看一下视频：<a href="https://www.coursera.org/learn/machine-learning/lecture/olRZo/unsupervised-learning" target="_blank" rel="noopener">Unsupervised Learning</a><br><img src="https://blog-1255898404.cos.ap-shanghai.myqcloud.com/ML_01/pic08.png" alt="pic08"></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;课程地址：&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/lecture/1VkCb/supervised-learning&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Supervised Learning &amp;amp; Unsupervised Learning&lt;/a&gt;&lt;br&gt;课程Wiki：&lt;a href=&quot;https://share.coursera.org/wiki/index.php/ML:Introduction&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Introduction&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&quot;监督学习&quot;&gt;&lt;a href=&quot;#监督学习&quot; class=&quot;headerlink&quot; title=&quot;监督学习&quot;&gt;&lt;/a&gt;监督学习&lt;/h4&gt;&lt;p&gt;什么是监督学习? 我们来看看维基百科中给出的定义:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;监督式学习（英语：Supervised learning），是一个机器学习中的方法，可以由训练资料中学到或建立一个模式（函数 / learning model），并依此模式推测新的实例。训练资料是由输入物件（通常是向量）和预期输出所组成。函数的输出可以是一个连续的值（称为回归分析），或是预测一个分类标签（称作分类）&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;从数据的角度来讲, 监督学习和无监督学习的区别就在于监督学习的数据不仅仅有特征组成, 即每一个数据样本都包含一个准确的输出值. 在房价预测的问题中, 数据由特征+房价组成。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="machinelearning" scheme="http://yoursite.com/tags/machinelearning/"/>
    
      <category term="math" scheme="http://yoursite.com/tags/math/"/>
    
      <category term="note" scheme="http://yoursite.com/tags/note/"/>
    
  </entry>
  
  <entry>
    <title>机器学习资料整理</title>
    <link href="http://yoursite.com/machine_learning_materials/"/>
    <id>http://yoursite.com/machine_learning_materials/</id>
    <published>2018-09-03T10:38:41.948Z</published>
    <updated>2018-09-03T11:26:46.850Z</updated>
    
    <content type="html"><![CDATA[<h4 id="微积分"><a href="#微积分" class="headerlink" title="微积分"></a>微积分</h4><p>&emsp;&emsp;<a href="https://ocw.mit.edu/courses/mathematics/18-01sc-single-variable-calculus-fall-2010/" target="_blank" rel="noopener">Single Variable Calculus - MIT</a></p><h4 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h4><p>&emsp;&emsp;<a href="https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/" target="_blank" rel="noopener">Linear Algebra - MIT</a><br><a id="more"></a></p><h4 id="概率论与数理统计"><a href="#概率论与数理统计" class="headerlink" title="概率论与数理统计"></a>概率论与数理统计</h4><p>&emsp;&emsp;(暂无)</p><h4 id="凸优化"><a href="#凸优化" class="headerlink" title="凸优化"></a>凸优化</h4><p>&emsp;&emsp;(暂无)</p><h4 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h4><p>&emsp;&emsp;<a href="https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000" target="_blank" rel="noopener">廖雪峰Python教程</a><br>&emsp;&emsp;<a href="https://learnxinyminutes.com/docs/zh-cn/python3-cn/" target="_blank" rel="noopener">Learn python in Y minutes</a></p><h4 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h4><p>&emsp;&emsp;<a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Coursera机器学习 - Andrew Ng</a><br>&emsp;&emsp;<a href="http://cs229.stanford.edu/" target="_blank" rel="noopener">CS229 - Andrew Ng</a><br>&emsp;&emsp;<a href="https://book.douban.com/subject/10590856/" target="_blank" rel="noopener">《统计学习方法》- 李航</a><br>&emsp;&emsp;<a href="https://book.douban.com/subject/26708119/" target="_blank" rel="noopener">《机器学习》 - 周志华</a><br>&emsp;&emsp;<a href="https://cn.udacity.com/mlnd" target="_blank" rel="noopener">Udacity Machine Learning Engineer Nanodegree</a></p><h4 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h4><p>&emsp;&emsp;<a href="https://www.coursera.org/specializations/deep-learning" target="_blank" rel="noopener">Coursera Deep Learning Sepecialization - deeplearning.ai</a></p><h4 id="自然语言系统"><a href="#自然语言系统" class="headerlink" title="自然语言系统"></a>自然语言系统</h4><p>&emsp;&emsp;(暂无)</p><h4 id="推荐系统"><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h4><p>&emsp;&emsp;<a href="https://book.douban.com/subject/10769749/" target="_blank" rel="noopener">推荐系统实践 - 项亮</a></p><h4 id="竞赛"><a href="#竞赛" class="headerlink" title="竞赛"></a>竞赛</h4><p>&emsp;&emsp;<a href="https://www.kaggle.com/" target="_blank" rel="noopener">Kaggle</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;微积分&quot;&gt;&lt;a href=&quot;#微积分&quot; class=&quot;headerlink&quot; title=&quot;微积分&quot;&gt;&lt;/a&gt;微积分&lt;/h4&gt;&lt;p&gt;&amp;emsp;&amp;emsp;&lt;a href=&quot;https://ocw.mit.edu/courses/mathematics/18-01sc-single-variable-calculus-fall-2010/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Single Variable Calculus - MIT&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;线性代数&quot;&gt;&lt;a href=&quot;#线性代数&quot; class=&quot;headerlink&quot; title=&quot;线性代数&quot;&gt;&lt;/a&gt;线性代数&lt;/h4&gt;&lt;p&gt;&amp;emsp;&amp;emsp;&lt;a href=&quot;https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Linear Algebra - MIT&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="machinelearning" scheme="http://yoursite.com/tags/machinelearning/"/>
    
      <category term="deeplearning" scheme="http://yoursite.com/tags/deeplearning/"/>
    
      <category term="math" scheme="http://yoursite.com/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>吴恩达机器学习笔记-目录</title>
    <link href="http://yoursite.com/machine_learning_note/"/>
    <id>http://yoursite.com/machine_learning_note/</id>
    <published>2018-09-03T09:47:33.890Z</published>
    <updated>2018-09-24T06:26:51.468Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>持续更新中……</p></blockquote><h5 id="Coursera机器学习笔记-一-监督学习vs无监督学习"><a href="#Coursera机器学习笔记-一-监督学习vs无监督学习" class="headerlink" title="Coursera机器学习笔记(一) - 监督学习vs无监督学习"></a><a href="https://bigmoutain.cn/machine_learning_note_01/" target="_blank" rel="noopener">Coursera机器学习笔记(一) - 监督学习vs无监督学习</a></h5><h5 id="Coursera机器学习笔记-二-单变量线性回归"><a href="#Coursera机器学习笔记-二-单变量线性回归" class="headerlink" title="Coursera机器学习笔记(二) - 单变量线性回归"></a><a href="https://bigmoutain.cn/machine_learning_note_02/" target="_blank" rel="noopener">Coursera机器学习笔记(二) - 单变量线性回归</a></h5><h5 id="Coursera机器学习笔记-三-多变量线性回归"><a href="#Coursera机器学习笔记-三-多变量线性回归" class="headerlink" title="Coursera机器学习笔记(三) - 多变量线性回归"></a><a href="https://bigmoutain.cn/machine_learning_note_03/" target="_blank" rel="noopener">Coursera机器学习笔记(三) - 多变量线性回归</a></h5><h5 id="Coursera机器学习笔记-四-Octave教程"><a href="#Coursera机器学习笔记-四-Octave教程" class="headerlink" title="Coursera机器学习笔记(四) - Octave教程"></a><a href="https://bigmoutain.cn/machine_learning_note_04/" target="_blank" rel="noopener">Coursera机器学习笔记(四) - Octave教程</a></h5><h5 id="Coursera机器学习笔记-五-Logistic-Regression"><a href="#Coursera机器学习笔记-五-Logistic-Regression" class="headerlink" title="Coursera机器学习笔记(五) - Logistic Regression"></a><a href="https://bigmoutain.cn/machine_learning_note_05/" target="_blank" rel="noopener">Coursera机器学习笔记(五) - Logistic Regression</a></h5><h5 id="Coursera机器学习笔记-六-正则化"><a href="#Coursera机器学习笔记-六-正则化" class="headerlink" title="Coursera机器学习笔记(六) - 正则化"></a><a href="https://bigmoutain.cn/machine_learning_note_06/" target="_blank" rel="noopener">Coursera机器学习笔记(六) - 正则化</a></h5><h5 id="Coursera机器学习笔记-七-神经网络-上"><a href="#Coursera机器学习笔记-七-神经网络-上" class="headerlink" title="Coursera机器学习笔记(七) - 神经网络(上)"></a><a href="https://bigmoutain.cn/machine_learning_note_07/" target="_blank" rel="noopener">Coursera机器学习笔记(七) - 神经网络(上)</a></h5>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;持续更新中……&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h5 id=&quot;Coursera机器学习笔记-一-监督学习vs无监督学习&quot;&gt;&lt;a href=&quot;#Coursera机器学习笔记-一-监督学习vs无监督学习&quot; class=&quot;headerlink&quot; t
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
      <category term="Math" scheme="http://yoursite.com/tags/Math/"/>
    
      <category term="Note" scheme="http://yoursite.com/tags/Note/"/>
    
  </entry>
  
</feed>
